{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p> Measure what changed in user behavior, not just which test to run. </p>"},{"location":"#outcome-effects","title":"Outcome Effects","text":"<p>pyexptest models experimental impact across three fundamental outcome dimensions:</p> Effect Type Question Answered Examples Conversion Whether something happens Signup, purchase, click, trial start Magnitude How much it happens Revenue, time spent, order value Timing When it happens Time to purchase, time to churn <p>This framework ensures experiments are interpreted in terms of behavioral change, not just statistical tests.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install pyexptest\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from pyexptest import conversion, magnitude, timing\n\n# Conversion: Did the treatment change whether users purchase?\nresult = conversion.analyze(\n    control_visitors=10000,\n    control_conversions=500,\n    variant_visitors=10000,\n    variant_conversions=600,\n)\nprint(f\"Conversion lift: {result.lift_percent:+.1f}%\")\n\n# Magnitude: Did the treatment change how much users spend?\nresult = magnitude.analyze(\n    control_visitors=5000,\n    control_mean=50.00,\n    control_std=25.00,\n    variant_visitors=5000,\n    variant_mean=52.50,\n    variant_std=25.00,\n)\nprint(f\"Revenue lift: ${result.lift_absolute:+.2f}\")\n\n# Timing: Did the treatment change when users convert?\nresult = timing.analyze(\n    control_times=[5, 8, 12, 15, 20],\n    control_events=[1, 1, 1, 0, 1],\n    treatment_times=[3, 6, 9, 12, 16],\n    treatment_events=[1, 1, 1, 1, 1],\n)\nprint(f\"Hazard ratio: {result.hazard_ratio:.2f}\")\n</code></pre> <p>Or use the fully-qualified path:</p> <pre><code>from pyexptest.effects.outcome import conversion, magnitude, timing\n</code></pre>"},{"location":"#conversion-effects-whether-it-happens","title":"\ud83d\udcca Conversion Effects \u2014 Whether it happens","text":"<p>Use when your outcome is binary: did the user convert or not?</p>"},{"location":"#analyze-a-test","title":"Analyze a Test","text":"<pre><code>from pyexptest import conversion\n\nresult = conversion.analyze(\n    control_visitors=10000,\n    control_conversions=500,      # 5.0% conversion\n    variant_visitors=10000,\n    variant_conversions=600,      # 6.0% conversion\n)\n\nprint(f\"Control: {result.control_rate:.2%}\")\nprint(f\"Variant: {result.variant_rate:.2%}\")\nprint(f\"Lift: {result.lift_percent:+.1f}%\")\nprint(f\"Significant: {result.is_significant}\")\nprint(f\"Winner: {result.winner}\")\n</code></pre>"},{"location":"#calculate-sample-size","title":"Calculate Sample Size","text":"<pre><code>plan = conversion.sample_size(\n    current_rate=5,       # 5% baseline\n    lift_percent=10,      # detect 10% relative lift\n    confidence=95,\n    power=80,\n)\n\nprint(f\"Need {plan.visitors_per_variant:,} per variant\")\nplan.with_daily_traffic(10000)\nprint(f\"Duration: {plan.test_duration_days} days\")\n</code></pre>"},{"location":"#multi-variant-tests-chi-square","title":"Multi-Variant Tests (Chi-Square)","text":"<pre><code>result = conversion.analyze_multi(\n    variants=[\n        {\"name\": \"control\", \"visitors\": 10000, \"conversions\": 500},\n        {\"name\": \"variant_a\", \"visitors\": 10000, \"conversions\": 550},\n        {\"name\": \"variant_b\", \"visitors\": 10000, \"conversions\": 600},\n    ]\n)\n\nprint(f\"Best: {result.best_variant}\")\nprint(f\"P-value: {result.p_value:.4f}\")\n</code></pre>"},{"location":"#difference-in-differences","title":"Difference-in-Differences","text":"<pre><code>result = conversion.diff_in_diff(\n    control_pre_visitors=5000, control_pre_conversions=250,\n    control_post_visitors=5000, control_post_conversions=275,\n    treatment_pre_visitors=5000, treatment_pre_conversions=250,\n    treatment_post_visitors=5000, treatment_post_conversions=350,\n)\n\nprint(f\"DiD effect: {result.diff_in_diff:+.2%}\")\n</code></pre>"},{"location":"#magnitude-effects-how-much-it-happens","title":"\ud83d\udcc8 Magnitude Effects \u2014 How much it happens","text":"<p>Use when your outcome is a continuous value: revenue, time, count.</p>"},{"location":"#analyze-a-test_1","title":"Analyze a Test","text":"<pre><code>from pyexptest import magnitude\n\nresult = magnitude.analyze(\n    control_visitors=5000,\n    control_mean=50.00,\n    control_std=25.00,\n    variant_visitors=5000,\n    variant_mean=52.50,\n    variant_std=25.00,\n)\n\nprint(f\"Control: ${result.control_mean:.2f}\")\nprint(f\"Variant: ${result.variant_mean:.2f}\")\nprint(f\"Lift: ${result.lift_absolute:+.2f} ({result.lift_percent:+.1f}%)\")\nprint(f\"Significant: {result.is_significant}\")\n</code></pre>"},{"location":"#calculate-sample-size_1","title":"Calculate Sample Size","text":"<pre><code>plan = magnitude.sample_size(\n    current_mean=50,      # $50 average\n    current_std=25,       # $25 std dev\n    lift_percent=5,       # detect 5% lift\n)\n\nprint(f\"Need {plan.visitors_per_variant:,} per variant\")\n</code></pre>"},{"location":"#multi-variant-tests-anova","title":"Multi-Variant Tests (ANOVA)","text":"<pre><code>result = magnitude.analyze_multi(\n    variants=[\n        {\"name\": \"control\", \"visitors\": 1000, \"mean\": 50, \"std\": 25},\n        {\"name\": \"new_layout\", \"visitors\": 1000, \"mean\": 52, \"std\": 25},\n        {\"name\": \"premium_upsell\", \"visitors\": 1000, \"mean\": 55, \"std\": 25},\n    ]\n)\n\nprint(f\"Best: {result.best_variant}\")\nprint(f\"F-statistic: {result.f_statistic:.2f}\")\n</code></pre>"},{"location":"#difference-in-differences_1","title":"Difference-in-Differences","text":"<pre><code>result = magnitude.diff_in_diff(\n    control_pre_n=1000, control_pre_mean=50, control_pre_std=25,\n    control_post_n=1000, control_post_mean=51, control_post_std=25,\n    treatment_pre_n=1000, treatment_pre_mean=50, treatment_pre_std=25,\n    treatment_post_n=1000, treatment_post_mean=55, treatment_post_std=26,\n)\n\nprint(f\"DiD effect: ${result.diff_in_diff:+.2f}\")\n</code></pre>"},{"location":"#timing-effects-when-it-happens","title":"\u23f1\ufe0f Timing Effects \u2014 When it happens","text":"<p>Use when you care about time-to-event: time to purchase, time to churn, event rates.</p>"},{"location":"#survival-analysis","title":"Survival Analysis","text":"<pre><code>from pyexptest import timing\n\nresult = timing.analyze(\n    control_times=[5, 8, 12, 15, 18, 22, 25, 30],\n    control_events=[1, 1, 1, 0, 1, 1, 0, 1],      # 1=event, 0=censored\n    treatment_times=[3, 6, 9, 12, 14, 16, 20, 24],\n    treatment_events=[1, 1, 1, 1, 0, 1, 1, 1],\n)\n\nprint(f\"Control median time: {result.control_median_time}\")\nprint(f\"Treatment median time: {result.treatment_median_time}\")\nprint(f\"Hazard ratio: {result.hazard_ratio:.3f}\")\nprint(f\"Significant: {result.is_significant}\")\n</code></pre>"},{"location":"#kaplan-meier-survival-curves","title":"Kaplan-Meier Survival Curves","text":"<pre><code>curve = timing.survival_curve(\n    times=[5, 10, 15, 20, 25, 30],\n    events=[1, 1, 0, 1, 1, 0],\n    confidence=95,\n)\n\nprint(f\"Median survival time: {curve.median_time}\")\nprint(f\"Survival probabilities: {curve.survival_probabilities}\")\n</code></pre>"},{"location":"#event-rate-analysis-poisson","title":"Event Rate Analysis (Poisson)","text":"<pre><code>result = timing.analyze_rates(\n    control_events=45,\n    control_exposure=100,      # 100 days of observation\n    treatment_events=38,\n    treatment_exposure=100,\n)\n\nprint(f\"Rate ratio: {result.rate_ratio:.3f}\")\nprint(f\"Significant: {result.is_significant}\")\n</code></pre>"},{"location":"#generate-stakeholder-reports","title":"\ud83d\udccb Generate Stakeholder Reports","text":"<p>Every effect type includes <code>summarize()</code> for markdown reports:</p> <pre><code>result = conversion.analyze(...)\nreport = conversion.summarize(result, test_name=\"Signup Button Test\")\nprint(report)\n</code></pre>"},{"location":"#web-interface","title":"\ud83c\udf10 Web Interface","text":"<p>pyexptest includes a web UI for interactive analysis:</p> <pre><code>pyexptest-server\n# Open http://localhost:8000\n</code></pre> <p>Features include:</p> <ul> <li>Sample Size Calculator \u2014 Plan tests with intuitive parameter explanations</li> <li>A/B Test Results \u2014 Analyze 2-variant and multi-variant tests</li> <li>Timing &amp; Rates \u2014 Survival analysis and Poisson rate comparisons</li> <li>Diff-in-Diff \u2014 Quasi-experimental causal inference</li> <li>Confidence Intervals \u2014 Estimate precision of your metrics</li> </ul>"},{"location":"#why-outcome-effects","title":"Why \"Outcome Effects\"?","text":"<p>Traditional A/B testing tools are test-centric: \"Which statistical test should I use?\"</p> <p>pyexptest is effect-centric: \"What changed about user behavior?\"</p> <p>This means:</p> <ol> <li>Matches how stakeholders think \u2014 \"Did conversion increase?\" not \"Did we reject the null hypothesis?\"</li> <li>Avoids false equivalence \u2014 A conversion effect and a magnitude effect are different things</li> <li>Generalizes naturally \u2014 Timing, variance, and durability effects fit cleanly</li> </ol>"},{"location":"#best-practices","title":"Best Practices","text":"<ol> <li>Decide sample size BEFORE starting \u2014 Don't peek and stop early</li> <li>Run for at least 1-2 weeks \u2014 Capture weekly patterns</li> <li>Look at confidence intervals \u2014 Not just p-values</li> <li>Statistical significance \u2260 business significance \u2014 A 0.1% lift might be \"significant\" but not worth it</li> <li>Use Bonferroni correction \u2014 For multi-variant tests</li> </ol>"},{"location":"#license","title":"License","text":"<p>MIT License</p>"},{"location":"#credits","title":"Credits","text":"<p>Inspired by Evan Miller's A/B Testing Tools.</p>"},{"location":"api/conversion/","title":"conversion","text":"<p>Conversion Effects \u2014 Whether something happens</p> <p>The <code>conversion</code> module provides tools for analyzing experiments where the outcome is binary: did the user convert or not? Use this for click rates, signup rates, purchase rates, and any yes/no metric.</p>"},{"location":"api/conversion/#overview","title":"Overview","text":"Function Purpose <code>sample_size()</code> Calculate required sample size for a test <code>analyze()</code> Analyze a 2-variant A/B test <code>analyze_multi()</code> Analyze a multi-variant test (3+ variants) <code>diff_in_diff()</code> Difference-in-Differences analysis <code>confidence_interval()</code> Calculate confidence interval for a rate <code>summarize()</code> Generate stakeholder report for 2-variant test <code>summarize_multi()</code> Generate stakeholder report for multi-variant test <code>summarize_diff_in_diff()</code> Generate stakeholder report for DiD <code>summarize_plan()</code> Generate stakeholder report for sample size plan"},{"location":"api/conversion/#sample_size","title":"sample_size","text":"<p>Calculate the required sample size to detect a given lift in conversion rate.</p> <pre><code>def sample_size(\n    current_rate: float,\n    lift_percent: float = 10,\n    confidence: int = 95,\n    power: int = 80,\n    num_variants: int = 2,\n) -&gt; SampleSizePlan\n</code></pre>"},{"location":"api/conversion/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>current_rate</code> <code>float</code> required Current conversion rate. Can be decimal (0.05) or percentage (5). <code>lift_percent</code> <code>float</code> <code>10</code> Minimum relative lift to detect (e.g., 10 = 10% improvement). <code>confidence</code> <code>int</code> <code>95</code> Confidence level (e.g., 95 for 95% confidence). <code>power</code> <code>int</code> <code>80</code> Statistical power (e.g., 80 for 80% power). <code>num_variants</code> <code>int</code> <code>2</code> Number of variants including control."},{"location":"api/conversion/#returns","title":"Returns","text":"<p><code>SampleSizePlan</code> with attributes:</p> Attribute Type Description <code>visitors_per_variant</code> <code>int</code> Required visitors per variant <code>total_visitors</code> <code>int</code> Total visitors needed across all variants <code>current_rate</code> <code>float</code> Current conversion rate (decimal) <code>expected_rate</code> <code>float</code> Expected variant rate if lift is achieved <code>lift_percent</code> <code>float</code> Target lift percentage <code>confidence</code> <code>int</code> Confidence level <code>power</code> <code>int</code> Statistical power <code>test_duration_days</code> <code>int | None</code> Estimated test duration (set via <code>with_daily_traffic()</code>)"},{"location":"api/conversion/#methods","title":"Methods","text":"<p><code>with_daily_traffic(daily_visitors: int) -&gt; SampleSizePlan</code></p> <p>Set daily traffic to calculate estimated test duration.</p>"},{"location":"api/conversion/#example","title":"Example","text":"<pre><code>from pyexptest import conversion\n\nplan = conversion.sample_size(\n    current_rate=5,       # 5% conversion rate\n    lift_percent=10,      # detect 10% relative lift\n    confidence=95,\n    power=80,\n)\n\nprint(f\"Need {plan.visitors_per_variant:,} per variant\")\nprint(f\"Total: {plan.total_visitors:,}\")\n\n# Calculate duration\nplan.with_daily_traffic(10000)\nprint(f\"Duration: {plan.test_duration_days} days\")\n</code></pre>"},{"location":"api/conversion/#analyze","title":"analyze","text":"<p>Analyze a 2-variant A/B test for conversion rates using a two-proportion z-test.</p> <pre><code>def analyze(\n    control_visitors: int,\n    control_conversions: int,\n    variant_visitors: int,\n    variant_conversions: int,\n    confidence: int = 95,\n) -&gt; TestResults\n</code></pre>"},{"location":"api/conversion/#parameters_1","title":"Parameters","text":"Parameter Type Default Description <code>control_visitors</code> <code>int</code> required Number of visitors in control group <code>control_conversions</code> <code>int</code> required Number of conversions in control group <code>variant_visitors</code> <code>int</code> required Number of visitors in variant group <code>variant_conversions</code> <code>int</code> required Number of conversions in variant group <code>confidence</code> <code>int</code> <code>95</code> Confidence level"},{"location":"api/conversion/#returns_1","title":"Returns","text":"<p><code>TestResults</code> with attributes:</p> Attribute Type Description <code>control_rate</code> <code>float</code> Control conversion rate <code>variant_rate</code> <code>float</code> Variant conversion rate <code>lift_percent</code> <code>float</code> Relative lift (%) <code>lift_absolute</code> <code>float</code> Absolute lift <code>is_significant</code> <code>bool</code> Whether result is statistically significant <code>confidence</code> <code>int</code> Confidence level used <code>p_value</code> <code>float</code> P-value of the test <code>confidence_interval_lower</code> <code>float</code> Lower bound of CI for lift <code>confidence_interval_upper</code> <code>float</code> Upper bound of CI for lift <code>winner</code> <code>str</code> <code>\"control\"</code>, <code>\"variant\"</code>, or <code>\"no winner yet\"</code> <code>recommendation</code> <code>str</code> Plain-English recommendation"},{"location":"api/conversion/#example_1","title":"Example","text":"<pre><code>from pyexptest import conversion\n\nresult = conversion.analyze(\n    control_visitors=10000,\n    control_conversions=500,\n    variant_visitors=10000,\n    variant_conversions=600,\n)\n\nprint(f\"Significant: {result.is_significant}\")\nprint(f\"Lift: {result.lift_percent:+.1f}%\")\nprint(f\"Winner: {result.winner}\")\nprint(result.recommendation)\n</code></pre>"},{"location":"api/conversion/#analyze_multi","title":"analyze_multi","text":"<p>Analyze a multi-variant test (3+ variants) using Chi-square test with optional Bonferroni correction for pairwise comparisons.</p> <pre><code>def analyze_multi(\n    variants: List[Dict[str, Any]],\n    confidence: int = 95,\n    correction: Literal[\"bonferroni\", \"none\"] = \"bonferroni\",\n) -&gt; MultiVariantResults\n</code></pre>"},{"location":"api/conversion/#parameters_2","title":"Parameters","text":"Parameter Type Default Description <code>variants</code> <code>list[dict]</code> required List of variant dictionaries <code>confidence</code> <code>int</code> <code>95</code> Confidence level <code>correction</code> <code>str</code> <code>\"bonferroni\"</code> Multiple comparison correction method <p>Each variant dictionary must have:</p> Key Type Description <code>name</code> <code>str</code> Variant name <code>visitors</code> <code>int</code> Number of visitors <code>conversions</code> <code>int</code> Number of conversions"},{"location":"api/conversion/#returns_2","title":"Returns","text":"<p><code>MultiVariantResults</code> with attributes:</p> Attribute Type Description <code>variants</code> <code>list[Variant]</code> List of Variant objects <code>is_significant</code> <code>bool</code> Whether overall test is significant <code>confidence</code> <code>int</code> Confidence level <code>p_value</code> <code>float</code> Chi-square test p-value <code>test_statistic</code> <code>float</code> Chi-square statistic <code>degrees_of_freedom</code> <code>int</code> Degrees of freedom <code>best_variant</code> <code>str</code> Name of best performing variant <code>worst_variant</code> <code>str</code> Name of worst performing variant <code>pairwise_comparisons</code> <code>list[PairwiseComparison]</code> All pairwise comparisons <code>recommendation</code> <code>str</code> Plain-English recommendation"},{"location":"api/conversion/#example_2","title":"Example","text":"<pre><code>from pyexptest import conversion\n\nresult = conversion.analyze_multi(\n    variants=[\n        {\"name\": \"control\", \"visitors\": 10000, \"conversions\": 500},\n        {\"name\": \"variant_a\", \"visitors\": 10000, \"conversions\": 550},\n        {\"name\": \"variant_b\", \"visitors\": 10000, \"conversions\": 600},\n    ]\n)\n\nprint(f\"Best: {result.best_variant}\")\nprint(f\"Significant: {result.is_significant}\")\n\nfor p in result.pairwise_comparisons:\n    if p.is_significant:\n        print(f\"  {p.variant_a} vs {p.variant_b}: p={p.p_value_adjusted:.4f}\")\n</code></pre>"},{"location":"api/conversion/#diff_in_diff","title":"diff_in_diff","text":"<p>Perform a Difference-in-Differences analysis for conversion rates. Used for quasi-experimental designs with pre/post measurements.</p> <pre><code>def diff_in_diff(\n    control_pre_visitors: int,\n    control_pre_conversions: int,\n    control_post_visitors: int,\n    control_post_conversions: int,\n    treatment_pre_visitors: int,\n    treatment_pre_conversions: int,\n    treatment_post_visitors: int,\n    treatment_post_conversions: int,\n    confidence: int = 95,\n) -&gt; DiffInDiffResults\n</code></pre>"},{"location":"api/conversion/#parameters_3","title":"Parameters","text":"Parameter Type Default Description <code>control_pre_visitors</code> <code>int</code> required Control group visitors in pre-period <code>control_pre_conversions</code> <code>int</code> required Control group conversions in pre-period <code>control_post_visitors</code> <code>int</code> required Control group visitors in post-period <code>control_post_conversions</code> <code>int</code> required Control group conversions in post-period <code>treatment_pre_visitors</code> <code>int</code> required Treatment group visitors in pre-period <code>treatment_pre_conversions</code> <code>int</code> required Treatment group conversions in pre-period <code>treatment_post_visitors</code> <code>int</code> required Treatment group visitors in post-period <code>treatment_post_conversions</code> <code>int</code> required Treatment group conversions in post-period <code>confidence</code> <code>int</code> <code>95</code> Confidence level"},{"location":"api/conversion/#returns_3","title":"Returns","text":"<p><code>DiffInDiffResults</code> with attributes:</p> Attribute Type Description <code>control_pre_rate</code> <code>float</code> Control pre-period conversion rate <code>control_post_rate</code> <code>float</code> Control post-period conversion rate <code>treatment_pre_rate</code> <code>float</code> Treatment pre-period conversion rate <code>treatment_post_rate</code> <code>float</code> Treatment post-period conversion rate <code>control_change</code> <code>float</code> Change in control group <code>treatment_change</code> <code>float</code> Change in treatment group <code>diff_in_diff</code> <code>float</code> DiD estimate (treatment effect) <code>diff_in_diff_percent</code> <code>float</code> DiD as relative percent <code>is_significant</code> <code>bool</code> Whether DiD is significant <code>confidence</code> <code>int</code> Confidence level <code>p_value</code> <code>float</code> P-value <code>z_statistic</code> <code>float</code> Z-statistic <code>confidence_interval_lower</code> <code>float</code> Lower CI bound <code>confidence_interval_upper</code> <code>float</code> Upper CI bound <code>recommendation</code> <code>str</code> Plain-English recommendation"},{"location":"api/conversion/#example_3","title":"Example","text":"<pre><code>from pyexptest import conversion\n\nresult = conversion.diff_in_diff(\n    control_pre_visitors=5000,\n    control_pre_conversions=250,\n    control_post_visitors=5000,\n    control_post_conversions=275,\n    treatment_pre_visitors=5000,\n    treatment_pre_conversions=250,\n    treatment_post_visitors=5000,\n    treatment_post_conversions=350,\n)\n\nprint(f\"DiD effect: {result.diff_in_diff:+.2%}\")\nprint(f\"Significant: {result.is_significant}\")\n</code></pre>"},{"location":"api/conversion/#confidence_interval","title":"confidence_interval","text":"<p>Calculate the confidence interval for a single conversion rate using the Wilson score interval.</p> <pre><code>def confidence_interval(\n    visitors: int,\n    conversions: int,\n    confidence: int = 95,\n) -&gt; ConfidenceInterval\n</code></pre>"},{"location":"api/conversion/#parameters_4","title":"Parameters","text":"Parameter Type Default Description <code>visitors</code> <code>int</code> required Number of visitors <code>conversions</code> <code>int</code> required Number of conversions <code>confidence</code> <code>int</code> <code>95</code> Confidence level"},{"location":"api/conversion/#returns_4","title":"Returns","text":"<p><code>ConfidenceInterval</code> with attributes:</p> Attribute Type Description <code>rate</code> <code>float</code> Observed conversion rate <code>lower</code> <code>float</code> Lower bound of CI <code>upper</code> <code>float</code> Upper bound of CI <code>confidence</code> <code>int</code> Confidence level <code>margin_of_error</code> <code>float</code> Margin of error"},{"location":"api/conversion/#example_4","title":"Example","text":"<pre><code>from pyexptest import conversion\n\nci = conversion.confidence_interval(\n    visitors=1000,\n    conversions=50,\n    confidence=95,\n)\n\nprint(f\"Rate: {ci.rate:.2%}\")\nprint(f\"95% CI: [{ci.lower:.2%}, {ci.upper:.2%}]\")\n</code></pre>"},{"location":"api/conversion/#summarize","title":"summarize","text":"<p>Generate a markdown report for a 2-variant test result.</p> <pre><code>def summarize(\n    result: TestResults,\n    test_name: str = \"A/B Test\",\n) -&gt; str\n</code></pre>"},{"location":"api/conversion/#parameters_5","title":"Parameters","text":"Parameter Type Default Description <code>result</code> <code>TestResults</code> required Result from <code>analyze()</code> <code>test_name</code> <code>str</code> <code>\"A/B Test\"</code> Name of the test for the report"},{"location":"api/conversion/#returns_5","title":"Returns","text":"<p>A markdown-formatted string suitable for sharing with stakeholders.</p>"},{"location":"api/conversion/#example_5","title":"Example","text":"<pre><code>from pyexptest import conversion\n\nresult = conversion.analyze(...)\nreport = conversion.summarize(result, test_name=\"Homepage CTA Test\")\nprint(report)\n</code></pre>"},{"location":"api/conversion/#summarize_multi","title":"summarize_multi","text":"<p>Generate a markdown report for a multi-variant test result.</p> <pre><code>def summarize_multi(\n    result: MultiVariantResults,\n    test_name: str = \"Multi-Variant Test\",\n) -&gt; str\n</code></pre>"},{"location":"api/conversion/#parameters_6","title":"Parameters","text":"Parameter Type Default Description <code>result</code> <code>MultiVariantResults</code> required Result from <code>analyze_multi()</code> <code>test_name</code> <code>str</code> <code>\"Multi-Variant Test\"</code> Name of the test for the report"},{"location":"api/conversion/#returns_6","title":"Returns","text":"<p>A markdown-formatted string with variant performance table and pairwise comparisons.</p>"},{"location":"api/conversion/#summarize_diff_in_diff","title":"summarize_diff_in_diff","text":"<p>Generate a markdown report for a Difference-in-Differences analysis.</p> <pre><code>def summarize_diff_in_diff(\n    result: DiffInDiffResults,\n    test_name: str = \"Difference-in-Differences Analysis\",\n) -&gt; str\n</code></pre>"},{"location":"api/conversion/#parameters_7","title":"Parameters","text":"Parameter Type Default Description <code>result</code> <code>DiffInDiffResults</code> required Result from <code>diff_in_diff()</code> <code>test_name</code> <code>str</code> <code>\"Difference-in-Differences Analysis\"</code> Name of the analysis"},{"location":"api/conversion/#returns_7","title":"Returns","text":"<p>A markdown-formatted string with pre/post comparison table, DiD estimate, and interpretation.</p>"},{"location":"api/conversion/#summarize_plan","title":"summarize_plan","text":"<p>Generate a markdown report for a sample size plan.</p> <pre><code>def summarize_plan(\n    plan: SampleSizePlan,\n    test_name: str = \"A/B Test\",\n) -&gt; str\n</code></pre>"},{"location":"api/conversion/#parameters_8","title":"Parameters","text":"Parameter Type Default Description <code>plan</code> <code>SampleSizePlan</code> required Result from <code>sample_size()</code> <code>test_name</code> <code>str</code> <code>\"A/B Test\"</code> Name of the test for the report"},{"location":"api/conversion/#returns_8","title":"Returns","text":"<p>A markdown-formatted string with test parameters, required sample size, and duration estimate.</p>"},{"location":"api/conversion/#data-classes","title":"Data Classes","text":""},{"location":"api/conversion/#samplesizeplan","title":"SampleSizePlan","text":"<pre><code>@dataclass\nclass SampleSizePlan:\n    visitors_per_variant: int\n    total_visitors: int\n    current_rate: float\n    expected_rate: float\n    lift_percent: float\n    confidence: int\n    power: int\n    test_duration_days: Optional[int] = None\n\n    def with_daily_traffic(self, daily_visitors: int) -&gt; 'SampleSizePlan': ...\n</code></pre>"},{"location":"api/conversion/#testresults","title":"TestResults","text":"<pre><code>@dataclass\nclass TestResults:\n    control_rate: float\n    variant_rate: float\n    lift_percent: float\n    lift_absolute: float\n    is_significant: bool\n    confidence: int\n    p_value: float\n    confidence_interval_lower: float\n    confidence_interval_upper: float\n    control_visitors: int\n    control_conversions: int\n    variant_visitors: int\n    variant_conversions: int\n    winner: Literal[\"control\", \"variant\", \"no winner yet\"]\n    recommendation: str\n</code></pre>"},{"location":"api/conversion/#confidenceinterval","title":"ConfidenceInterval","text":"<pre><code>@dataclass\nclass ConfidenceInterval:\n    rate: float\n    lower: float\n    upper: float\n    confidence: int\n    margin_of_error: float\n</code></pre>"},{"location":"api/conversion/#multivariantresults","title":"MultiVariantResults","text":"<pre><code>@dataclass\nclass MultiVariantResults:\n    variants: List[Variant]\n    is_significant: bool\n    confidence: int\n    p_value: float\n    test_statistic: float\n    degrees_of_freedom: int\n    best_variant: str\n    worst_variant: str\n    pairwise_comparisons: List[PairwiseComparison]\n    recommendation: str\n</code></pre>"},{"location":"api/conversion/#pairwisecomparison","title":"PairwiseComparison","text":"<pre><code>@dataclass\nclass PairwiseComparison:\n    variant_a: str\n    variant_b: str\n    rate_a: float\n    rate_b: float\n    lift_percent: float\n    lift_absolute: float\n    p_value: float\n    p_value_adjusted: float\n    is_significant: bool\n    confidence_interval_lower: float\n    confidence_interval_upper: float\n</code></pre>"},{"location":"api/magnitude/","title":"magnitude","text":"<p>Magnitude Effects \u2014 How much it happens</p> <p>The <code>magnitude</code> module provides tools for analyzing experiments where the outcome is a continuous value: revenue, time spent, order value, number of actions. Use this when you care about the size of the outcome, not just whether it happened.</p>"},{"location":"api/magnitude/#overview","title":"Overview","text":"Function Purpose <code>sample_size()</code> Calculate required sample size for a test <code>analyze()</code> Analyze a 2-variant A/B test <code>analyze_multi()</code> Analyze a multi-variant test (3+ variants) <code>diff_in_diff()</code> Difference-in-Differences analysis <code>confidence_interval()</code> Calculate confidence interval for a mean <code>summarize()</code> Generate stakeholder report for 2-variant test <code>summarize_multi()</code> Generate stakeholder report for multi-variant test <code>summarize_diff_in_diff()</code> Generate stakeholder report for DiD <code>summarize_plan()</code> Generate stakeholder report for sample size plan"},{"location":"api/magnitude/#sample_size","title":"sample_size","text":"<p>Calculate the required sample size to detect a given lift in a numeric metric.</p> <pre><code>def sample_size(\n    current_mean: float,\n    current_std: float,\n    lift_percent: float = 5,\n    confidence: int = 95,\n    power: int = 80,\n    num_variants: int = 2,\n) -&gt; SampleSizePlan\n</code></pre>"},{"location":"api/magnitude/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>current_mean</code> <code>float</code> required Current mean value of the metric <code>current_std</code> <code>float</code> required Standard deviation of the metric <code>lift_percent</code> <code>float</code> <code>5</code> Minimum relative lift to detect (e.g., 5 = 5% improvement) <code>confidence</code> <code>int</code> <code>95</code> Confidence level (e.g., 95 for 95% confidence) <code>power</code> <code>int</code> <code>80</code> Statistical power (e.g., 80 for 80% power) <code>num_variants</code> <code>int</code> <code>2</code> Number of variants including control"},{"location":"api/magnitude/#returns","title":"Returns","text":"<p><code>SampleSizePlan</code> with attributes:</p> Attribute Type Description <code>visitors_per_variant</code> <code>int</code> Required visitors per variant <code>total_visitors</code> <code>int</code> Total visitors needed across all variants <code>current_mean</code> <code>float</code> Current mean value <code>expected_mean</code> <code>float</code> Expected variant mean if lift is achieved <code>standard_deviation</code> <code>float</code> Standard deviation used <code>lift_percent</code> <code>float</code> Target lift percentage <code>confidence</code> <code>int</code> Confidence level <code>power</code> <code>int</code> Statistical power <code>test_duration_days</code> <code>int | None</code> Estimated test duration (set via <code>with_daily_traffic()</code>)"},{"location":"api/magnitude/#methods","title":"Methods","text":"<p><code>with_daily_traffic(daily_visitors: int) -&gt; SampleSizePlan</code></p> <p>Set daily traffic to calculate estimated test duration.</p>"},{"location":"api/magnitude/#example","title":"Example","text":"<pre><code>from pyexptest import magnitude\n\nplan = magnitude.sample_size(\n    current_mean=50,      # $50 average order value\n    current_std=25,       # $25 standard deviation\n    lift_percent=5,       # detect 5% relative lift\n    confidence=95,\n    power=80,\n)\n\nprint(f\"Need {plan.visitors_per_variant:,} per variant\")\nprint(f\"Total: {plan.total_visitors:,}\")\n\n# Calculate duration\nplan.with_daily_traffic(5000)\nprint(f\"Duration: {plan.test_duration_days} days\")\n</code></pre>"},{"location":"api/magnitude/#analyze","title":"analyze","text":"<p>Analyze a 2-variant A/B test for numeric metrics using Welch's t-test.</p> <pre><code>def analyze(\n    control_visitors: int,\n    control_mean: float,\n    control_std: float,\n    variant_visitors: int,\n    variant_mean: float,\n    variant_std: float,\n    confidence: int = 95,\n) -&gt; TestResults\n</code></pre>"},{"location":"api/magnitude/#parameters_1","title":"Parameters","text":"Parameter Type Default Description <code>control_visitors</code> <code>int</code> required Sample size in control group <code>control_mean</code> <code>float</code> required Mean value in control group <code>control_std</code> <code>float</code> required Standard deviation in control group <code>variant_visitors</code> <code>int</code> required Sample size in variant group <code>variant_mean</code> <code>float</code> required Mean value in variant group <code>variant_std</code> <code>float</code> required Standard deviation in variant group <code>confidence</code> <code>int</code> <code>95</code> Confidence level"},{"location":"api/magnitude/#returns_1","title":"Returns","text":"<p><code>TestResults</code> with attributes:</p> Attribute Type Description <code>control_mean</code> <code>float</code> Control mean <code>variant_mean</code> <code>float</code> Variant mean <code>lift_percent</code> <code>float</code> Relative lift (%) <code>lift_absolute</code> <code>float</code> Absolute lift <code>is_significant</code> <code>bool</code> Whether result is statistically significant <code>confidence</code> <code>int</code> Confidence level used <code>p_value</code> <code>float</code> P-value of the test <code>confidence_interval_lower</code> <code>float</code> Lower bound of CI for lift <code>confidence_interval_upper</code> <code>float</code> Upper bound of CI for lift <code>control_std</code> <code>float</code> Control standard deviation <code>variant_std</code> <code>float</code> Variant standard deviation <code>winner</code> <code>str</code> <code>\"control\"</code>, <code>\"variant\"</code>, or <code>\"no winner yet\"</code> <code>recommendation</code> <code>str</code> Plain-English recommendation"},{"location":"api/magnitude/#example_1","title":"Example","text":"<pre><code>from pyexptest import magnitude\n\nresult = magnitude.analyze(\n    control_visitors=5000,\n    control_mean=50.00,\n    control_std=25.00,\n    variant_visitors=5000,\n    variant_mean=52.50,\n    variant_std=25.00,\n)\n\nprint(f\"Significant: {result.is_significant}\")\nprint(f\"Lift: {result.lift_percent:+.1f}%\")\nprint(f\"Winner: {result.winner}\")\nprint(result.recommendation)\n</code></pre>"},{"location":"api/magnitude/#analyze_multi","title":"analyze_multi","text":"<p>Analyze a multi-variant test (3+ variants) using one-way ANOVA with optional Bonferroni correction for pairwise comparisons.</p> <pre><code>def analyze_multi(\n    variants: List[Dict[str, Any]],\n    confidence: int = 95,\n    correction: Literal[\"bonferroni\", \"none\"] = \"bonferroni\",\n) -&gt; MultiVariantResults\n</code></pre>"},{"location":"api/magnitude/#parameters_2","title":"Parameters","text":"Parameter Type Default Description <code>variants</code> <code>list[dict]</code> required List of variant dictionaries <code>confidence</code> <code>int</code> <code>95</code> Confidence level <code>correction</code> <code>str</code> <code>\"bonferroni\"</code> Multiple comparison correction method <p>Each variant dictionary must have:</p> Key Type Description <code>name</code> <code>str</code> Variant name <code>visitors</code> <code>int</code> Sample size <code>mean</code> <code>float</code> Mean value <code>std</code> <code>float</code> Standard deviation"},{"location":"api/magnitude/#returns_2","title":"Returns","text":"<p><code>MultiVariantResults</code> with attributes:</p> Attribute Type Description <code>variants</code> <code>list[Variant]</code> List of Variant objects <code>is_significant</code> <code>bool</code> Whether overall test is significant <code>confidence</code> <code>int</code> Confidence level <code>p_value</code> <code>float</code> ANOVA test p-value <code>f_statistic</code> <code>float</code> F-statistic <code>df_between</code> <code>int</code> Degrees of freedom (between groups) <code>df_within</code> <code>int</code> Degrees of freedom (within groups) <code>best_variant</code> <code>str</code> Name of best performing variant <code>worst_variant</code> <code>str</code> Name of worst performing variant <code>pairwise_comparisons</code> <code>list[PairwiseComparison]</code> All pairwise comparisons <code>recommendation</code> <code>str</code> Plain-English recommendation"},{"location":"api/magnitude/#example_2","title":"Example","text":"<pre><code>from pyexptest import magnitude\n\nresult = magnitude.analyze_multi(\n    variants=[\n        {\"name\": \"control\", \"visitors\": 1000, \"mean\": 50, \"std\": 25},\n        {\"name\": \"new_layout\", \"visitors\": 1000, \"mean\": 52, \"std\": 25},\n        {\"name\": \"premium_upsell\", \"visitors\": 1000, \"mean\": 55, \"std\": 25},\n    ]\n)\n\nprint(f\"Best: {result.best_variant}\")\nprint(f\"F-statistic: {result.f_statistic:.2f}\")\nprint(f\"Significant: {result.is_significant}\")\n\nfor p in result.pairwise_comparisons:\n    if p.is_significant:\n        print(f\"  {p.variant_a} vs {p.variant_b}: p={p.p_value_adjusted:.4f}\")\n</code></pre>"},{"location":"api/magnitude/#diff_in_diff","title":"diff_in_diff","text":"<p>Perform a Difference-in-Differences analysis for numeric metrics. Used for quasi-experimental designs with pre/post measurements.</p> <pre><code>def diff_in_diff(\n    control_pre_n: int,\n    control_pre_mean: float,\n    control_pre_std: float,\n    control_post_n: int,\n    control_post_mean: float,\n    control_post_std: float,\n    treatment_pre_n: int,\n    treatment_pre_mean: float,\n    treatment_pre_std: float,\n    treatment_post_n: int,\n    treatment_post_mean: float,\n    treatment_post_std: float,\n    confidence: int = 95,\n) -&gt; DiffInDiffResults\n</code></pre>"},{"location":"api/magnitude/#parameters_3","title":"Parameters","text":"Parameter Type Default Description <code>control_pre_n</code> <code>int</code> required Control group sample size in pre-period <code>control_pre_mean</code> <code>float</code> required Control group mean in pre-period <code>control_pre_std</code> <code>float</code> required Control group std dev in pre-period <code>control_post_n</code> <code>int</code> required Control group sample size in post-period <code>control_post_mean</code> <code>float</code> required Control group mean in post-period <code>control_post_std</code> <code>float</code> required Control group std dev in post-period <code>treatment_pre_n</code> <code>int</code> required Treatment group sample size in pre-period <code>treatment_pre_mean</code> <code>float</code> required Treatment group mean in pre-period <code>treatment_pre_std</code> <code>float</code> required Treatment group std dev in pre-period <code>treatment_post_n</code> <code>int</code> required Treatment group sample size in post-period <code>treatment_post_mean</code> <code>float</code> required Treatment group mean in post-period <code>treatment_post_std</code> <code>float</code> required Treatment group std dev in post-period <code>confidence</code> <code>int</code> <code>95</code> Confidence level"},{"location":"api/magnitude/#returns_3","title":"Returns","text":"<p><code>DiffInDiffResults</code> with attributes:</p> Attribute Type Description <code>control_pre_mean</code> <code>float</code> Control pre-period mean <code>control_post_mean</code> <code>float</code> Control post-period mean <code>treatment_pre_mean</code> <code>float</code> Treatment pre-period mean <code>treatment_post_mean</code> <code>float</code> Treatment post-period mean <code>control_change</code> <code>float</code> Change in control group <code>treatment_change</code> <code>float</code> Change in treatment group <code>diff_in_diff</code> <code>float</code> DiD estimate (treatment effect) <code>diff_in_diff_percent</code> <code>float</code> DiD as relative percent <code>is_significant</code> <code>bool</code> Whether DiD is significant <code>confidence</code> <code>int</code> Confidence level <code>p_value</code> <code>float</code> P-value <code>t_statistic</code> <code>float</code> T-statistic <code>degrees_of_freedom</code> <code>float</code> Degrees of freedom <code>confidence_interval_lower</code> <code>float</code> Lower CI bound <code>confidence_interval_upper</code> <code>float</code> Upper CI bound <code>recommendation</code> <code>str</code> Plain-English recommendation"},{"location":"api/magnitude/#example_3","title":"Example","text":"<pre><code>from pyexptest import magnitude\n\nresult = magnitude.diff_in_diff(\n    control_pre_n=1000,\n    control_pre_mean=50.00,\n    control_pre_std=25.00,\n    control_post_n=1000,\n    control_post_mean=51.00,\n    control_post_std=25.00,\n    treatment_pre_n=1000,\n    treatment_pre_mean=50.00,\n    treatment_pre_std=25.00,\n    treatment_post_n=1000,\n    treatment_post_mean=55.00,\n    treatment_post_std=26.00,\n)\n\nprint(f\"DiD effect: ${result.diff_in_diff:+.2f}\")\nprint(f\"Significant: {result.is_significant}\")\n</code></pre>"},{"location":"api/magnitude/#confidence_interval","title":"confidence_interval","text":"<p>Calculate the confidence interval for a single mean using the t-distribution.</p> <pre><code>def confidence_interval(\n    visitors: int,\n    mean: float,\n    std: float,\n    confidence: int = 95,\n) -&gt; ConfidenceInterval\n</code></pre>"},{"location":"api/magnitude/#parameters_4","title":"Parameters","text":"Parameter Type Default Description <code>visitors</code> <code>int</code> required Sample size <code>mean</code> <code>float</code> required Sample mean <code>std</code> <code>float</code> required Sample standard deviation <code>confidence</code> <code>int</code> <code>95</code> Confidence level"},{"location":"api/magnitude/#returns_4","title":"Returns","text":"<p><code>ConfidenceInterval</code> with attributes:</p> Attribute Type Description <code>mean</code> <code>float</code> Sample mean <code>lower</code> <code>float</code> Lower bound of CI <code>upper</code> <code>float</code> Upper bound of CI <code>confidence</code> <code>int</code> Confidence level <code>margin_of_error</code> <code>float</code> Margin of error"},{"location":"api/magnitude/#example_4","title":"Example","text":"<pre><code>from pyexptest import magnitude\n\nci = magnitude.confidence_interval(\n    visitors=1000,\n    mean=50.00,\n    std=25.00,\n    confidence=95,\n)\n\nprint(f\"Mean: ${ci.mean:.2f}\")\nprint(f\"95% CI: [${ci.lower:.2f}, ${ci.upper:.2f}]\")\n</code></pre>"},{"location":"api/magnitude/#summarize","title":"summarize","text":"<p>Generate a markdown report for a 2-variant test result.</p> <pre><code>def summarize(\n    result: TestResults,\n    test_name: str = \"Revenue Test\",\n    metric_name: str = \"Average Order Value\",\n    currency: str = \"$\",\n) -&gt; str\n</code></pre>"},{"location":"api/magnitude/#parameters_5","title":"Parameters","text":"Parameter Type Default Description <code>result</code> <code>TestResults</code> required Result from <code>analyze()</code> <code>test_name</code> <code>str</code> <code>\"Revenue Test\"</code> Name of the test for the report <code>metric_name</code> <code>str</code> <code>\"Average Order Value\"</code> Name of the metric <code>currency</code> <code>str</code> <code>\"$\"</code> Currency symbol to use"},{"location":"api/magnitude/#returns_5","title":"Returns","text":"<p>A markdown-formatted string suitable for sharing with stakeholders.</p>"},{"location":"api/magnitude/#example_5","title":"Example","text":"<pre><code>from pyexptest import magnitude\n\nresult = magnitude.analyze(...)\nreport = magnitude.summarize(\n    result,\n    test_name=\"Checkout Flow Test\",\n    metric_name=\"Average Order Value\",\n    currency=\"$\"\n)\nprint(report)\n</code></pre>"},{"location":"api/magnitude/#summarize_multi","title":"summarize_multi","text":"<p>Generate a markdown report for a multi-variant test result.</p> <pre><code>def summarize_multi(\n    result: MultiVariantResults,\n    test_name: str = \"Multi-Variant Test\",\n    metric_name: str = \"Average Value\",\n    currency: str = \"$\",\n) -&gt; str\n</code></pre>"},{"location":"api/magnitude/#parameters_6","title":"Parameters","text":"Parameter Type Default Description <code>result</code> <code>MultiVariantResults</code> required Result from <code>analyze_multi()</code> <code>test_name</code> <code>str</code> <code>\"Multi-Variant Test\"</code> Name of the test for the report <code>metric_name</code> <code>str</code> <code>\"Average Value\"</code> Name of the metric <code>currency</code> <code>str</code> <code>\"$\"</code> Currency symbol to use"},{"location":"api/magnitude/#returns_6","title":"Returns","text":"<p>A markdown-formatted string with variant performance table and pairwise comparisons.</p>"},{"location":"api/magnitude/#summarize_diff_in_diff","title":"summarize_diff_in_diff","text":"<p>Generate a markdown report for a Difference-in-Differences analysis.</p> <pre><code>def summarize_diff_in_diff(\n    result: DiffInDiffResults,\n    test_name: str = \"Difference-in-Differences Analysis\",\n    metric_name: str = \"Average Value\",\n    currency: str = \"$\",\n) -&gt; str\n</code></pre>"},{"location":"api/magnitude/#parameters_7","title":"Parameters","text":"Parameter Type Default Description <code>result</code> <code>DiffInDiffResults</code> required Result from <code>diff_in_diff()</code> <code>test_name</code> <code>str</code> <code>\"Difference-in-Differences Analysis\"</code> Name of the analysis <code>metric_name</code> <code>str</code> <code>\"Average Value\"</code> Name of the metric <code>currency</code> <code>str</code> <code>\"$\"</code> Currency symbol"},{"location":"api/magnitude/#returns_7","title":"Returns","text":"<p>A markdown-formatted string with pre/post comparison table, DiD estimate, and interpretation.</p>"},{"location":"api/magnitude/#summarize_plan","title":"summarize_plan","text":"<p>Generate a markdown report for a sample size plan.</p> <pre><code>def summarize_plan(\n    plan: SampleSizePlan,\n    test_name: str = \"Revenue Test\",\n    metric_name: str = \"Average Order Value\",\n    currency: str = \"$\",\n) -&gt; str\n</code></pre>"},{"location":"api/magnitude/#parameters_8","title":"Parameters","text":"Parameter Type Default Description <code>plan</code> <code>SampleSizePlan</code> required Result from <code>sample_size()</code> <code>test_name</code> <code>str</code> <code>\"Revenue Test\"</code> Name of the test for the report <code>metric_name</code> <code>str</code> <code>\"Average Order Value\"</code> Name of the metric <code>currency</code> <code>str</code> <code>\"$\"</code> Currency symbol to use"},{"location":"api/magnitude/#returns_8","title":"Returns","text":"<p>A markdown-formatted string with test parameters, required sample size, and duration estimate.</p>"},{"location":"api/magnitude/#data-classes","title":"Data Classes","text":""},{"location":"api/magnitude/#samplesizeplan","title":"SampleSizePlan","text":"<pre><code>@dataclass\nclass SampleSizePlan:\n    visitors_per_variant: int\n    total_visitors: int\n    current_mean: float\n    expected_mean: float\n    standard_deviation: float\n    lift_percent: float\n    confidence: int\n    power: int\n    test_duration_days: Optional[int] = None\n\n    def with_daily_traffic(self, daily_visitors: int) -&gt; 'SampleSizePlan': ...\n</code></pre>"},{"location":"api/magnitude/#testresults","title":"TestResults","text":"<pre><code>@dataclass\nclass TestResults:\n    control_mean: float\n    variant_mean: float\n    lift_percent: float\n    lift_absolute: float\n    is_significant: bool\n    confidence: int\n    p_value: float\n    confidence_interval_lower: float\n    confidence_interval_upper: float\n    control_visitors: int\n    control_std: float\n    variant_visitors: int\n    variant_std: float\n    winner: Literal[\"control\", \"variant\", \"no winner yet\"]\n    recommendation: str\n</code></pre>"},{"location":"api/magnitude/#confidenceinterval","title":"ConfidenceInterval","text":"<pre><code>@dataclass\nclass ConfidenceInterval:\n    mean: float\n    lower: float\n    upper: float\n    confidence: int\n    margin_of_error: float\n</code></pre>"},{"location":"api/magnitude/#multivariantresults","title":"MultiVariantResults","text":"<pre><code>@dataclass\nclass MultiVariantResults:\n    variants: List[Variant]\n    is_significant: bool\n    confidence: int\n    p_value: float\n    f_statistic: float\n    df_between: int\n    df_within: int\n    best_variant: str\n    worst_variant: str\n    pairwise_comparisons: List[PairwiseComparison]\n    recommendation: str\n</code></pre>"},{"location":"api/magnitude/#pairwisecomparison","title":"PairwiseComparison","text":"<pre><code>@dataclass\nclass PairwiseComparison:\n    variant_a: str\n    variant_b: str\n    mean_a: float\n    mean_b: float\n    lift_percent: float\n    lift_absolute: float\n    p_value: float\n    p_value_adjusted: float\n    is_significant: bool\n    confidence_interval_lower: float\n    confidence_interval_upper: float\n</code></pre>"},{"location":"api/timing/","title":"timing","text":"<p>Timing Effects \u2014 When it happens</p> <p>The <code>timing</code> module provides tools for analyzing experiments where you care about when an event occurs or how often events happen.</p>"},{"location":"api/timing/#use-cases","title":"Use Cases","text":"<ul> <li>Time to first purchase \u2014 Does a welcome email speed up first purchase?</li> <li>Time to churn \u2014 Does a new feature reduce churn rate?</li> <li>Time to activation \u2014 Does onboarding UX speed up activation?</li> <li>Support ticket rate \u2014 Does a UI change reduce support requests?</li> <li>Error rate \u2014 Does a code change reduce error frequency?</li> </ul>"},{"location":"api/timing/#survival-analysis","title":"Survival Analysis","text":"<p>Compare time-to-event between two groups using log-rank tests and hazard ratios.</p>"},{"location":"api/timing/#analyze","title":"analyze()","text":"<pre><code>from pyexptest import timing\n\nresult = timing.analyze(\n    control_times=[5, 8, 12, 15, 18, 22, 25, 30, 35, 40],\n    control_events=[1, 1, 1, 0, 1, 1, 0, 1, 0, 1],\n    treatment_times=[3, 6, 9, 12, 14, 16, 20, 24, 28, 32],\n    treatment_events=[1, 1, 1, 1, 0, 1, 1, 0, 1, 1],\n    confidence=95,\n)\n\nprint(f\"Control median time: {result.control_median_time}\")\nprint(f\"Treatment median time: {result.treatment_median_time}\")\nprint(f\"Hazard ratio: {result.hazard_ratio:.3f}\")\nprint(f\"HR 95% CI: [{result.hazard_ratio_ci_lower:.3f}, {result.hazard_ratio_ci_upper:.3f}]\")\nprint(f\"Time saved: {result.time_saved:.1f} ({result.time_saved_percent:.1f}%)\")\nprint(f\"P-value: {result.p_value:.4f}\")\nprint(f\"Significant: {result.is_significant}\")\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>control_times</code> List[float] Time values for each control subject <code>control_events</code> List[int] Event indicators (1=event, 0=censored) <code>treatment_times</code> List[float] Time values for each treatment subject <code>treatment_events</code> List[int] Event indicators (1=event, 0=censored) <code>confidence</code> int Confidence level (default: 95) <p>Returns: <code>TimingResults</code></p> Attribute Type Description <code>control_median_time</code> float Median time for control (None if not reached) <code>treatment_median_time</code> float Median time for treatment (None if not reached) <code>control_events</code> int Number of events in control <code>control_censored</code> int Number censored in control <code>treatment_events</code> int Number of events in treatment <code>treatment_censored</code> int Number censored in treatment <code>hazard_ratio</code> float Hazard ratio (treatment / control) <code>hazard_ratio_ci_lower</code> float Lower bound of HR confidence interval <code>hazard_ratio_ci_upper</code> float Upper bound of HR confidence interval <code>time_saved</code> float Difference in median times <code>time_saved_percent</code> float Percentage time difference <code>is_significant</code> bool Whether the difference is significant <code>p_value</code> float P-value from log-rank test <code>recommendation</code> str Plain-language interpretation"},{"location":"api/timing/#interpreting-hazard-ratios","title":"Interpreting Hazard Ratios","text":"HR Value Interpretation HR &lt; 1 Treatment slows down the event (protective effect) HR = 1 No effect on timing HR &gt; 1 Treatment speeds up the event <p>Example: HR = 0.7 means the treatment reduces the event rate by 30%.</p>"},{"location":"api/timing/#kaplan-meier-survival-curves","title":"Kaplan-Meier Survival Curves","text":""},{"location":"api/timing/#survival_curve","title":"survival_curve()","text":"<p>Generate survival probability estimates over time.</p> <pre><code>curve = timing.survival_curve(\n    times=[5, 10, 15, 20, 25, 30],\n    events=[1, 1, 0, 1, 1, 0],\n    confidence=95,\n)\n\nprint(f\"Times: {curve.times}\")\nprint(f\"Survival probabilities: {curve.survival_probabilities}\")\nprint(f\"Median survival time: {curve.median_time}\")\nprint(f\"Total events: {curve.events}\")\nprint(f\"Total censored: {curve.censored}\")\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>times</code> List[float] Time values for each subject <code>events</code> List[int] Event indicators (1=event, 0=censored) <code>confidence</code> int Confidence level for CI bands (default: 95) <p>Returns: <code>SurvivalCurve</code></p> Attribute Type Description <code>times</code> List[float] Time points <code>survival_probabilities</code> List[float] Survival probability at each time <code>confidence_lower</code> List[float] Lower CI bound <code>confidence_upper</code> List[float] Upper CI bound <code>median_time</code> float Median survival time (None if not reached) <code>events</code> int Total number of events <code>censored</code> int Total number censored <code>total</code> int Total sample size"},{"location":"api/timing/#event-rate-analysis-poisson","title":"Event Rate Analysis (Poisson)","text":""},{"location":"api/timing/#analyze_rates","title":"analyze_rates()","text":"<p>Compare event rates between two groups.</p> <pre><code>result = timing.analyze_rates(\n    control_events=45,\n    control_exposure=100,      # e.g., 100 person-days\n    treatment_events=38,\n    treatment_exposure=100,\n    confidence=95,\n)\n\nprint(f\"Control rate: {result.control_rate:.4f} events/unit\")\nprint(f\"Treatment rate: {result.treatment_rate:.4f} events/unit\")\nprint(f\"Rate ratio: {result.rate_ratio:.3f}\")\nprint(f\"RR 95% CI: [{result.rate_ratio_ci_lower:.3f}, {result.rate_ratio_ci_upper:.3f}]\")\nprint(f\"Rate change: {result.rate_difference_percent:+.1f}%\")\nprint(f\"P-value: {result.p_value:.4f}\")\nprint(f\"Significant: {result.is_significant}\")\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>control_events</code> int Number of events in control <code>control_exposure</code> float Total exposure time for control <code>treatment_events</code> int Number of events in treatment <code>treatment_exposure</code> float Total exposure time for treatment <code>confidence</code> int Confidence level (default: 95) <p>Returns: <code>RateResults</code></p> Attribute Type Description <code>control_rate</code> float Event rate in control (events/exposure) <code>treatment_rate</code> float Event rate in treatment <code>rate_ratio</code> float Rate ratio (treatment / control) <code>rate_ratio_ci_lower</code> float Lower bound of RR confidence interval <code>rate_ratio_ci_upper</code> float Upper bound of RR confidence interval <code>rate_difference</code> float Absolute difference in rates <code>rate_difference_percent</code> float Percentage change in rate <code>is_significant</code> bool Whether the difference is significant <code>p_value</code> float P-value from chi-square test <code>recommendation</code> str Plain-language interpretation"},{"location":"api/timing/#interpreting-rate-ratios","title":"Interpreting Rate Ratios","text":"RR Value Interpretation RR &lt; 1 Treatment reduces the event rate RR = 1 No effect on rate RR &gt; 1 Treatment increases the event rate <p>Example: RR = 0.85 means the treatment reduces events by 15%.</p>"},{"location":"api/timing/#sample-size-planning","title":"Sample Size Planning","text":""},{"location":"api/timing/#sample_size","title":"sample_size()","text":"<p>Calculate required sample size for a survival study.</p> <pre><code>plan = timing.sample_size(\n    control_median=30,        # Expected median for control\n    treatment_median=24,      # Expected median for treatment (20% faster)\n    confidence=95,\n    power=80,\n    dropout_rate=0.1,         # 10% expected censoring\n)\n\nprint(f\"Subjects per group: {plan.subjects_per_group:,}\")\nprint(f\"Total subjects: {plan.total_subjects:,}\")\nprint(f\"Expected events per group: {plan.expected_events_per_group:,}\")\nprint(f\"Total expected events: {plan.total_expected_events:,}\")\nprint(f\"Hazard ratio to detect: {plan.hazard_ratio:.3f}\")\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>control_median</code> float Expected median survival time for control <code>treatment_median</code> float Expected median survival time for treatment <code>confidence</code> int Confidence level (default: 95) <code>power</code> int Statistical power (default: 80) <code>dropout_rate</code> float Expected censoring rate (default: 0.1) <p>Returns: <code>TimingSampleSizePlan</code></p> Attribute Type Description <code>subjects_per_group</code> int Required subjects per group <code>total_subjects</code> int Total required subjects <code>expected_events_per_group</code> int Expected events per group <code>total_expected_events</code> int Total expected events <code>control_median</code> float Control median used <code>treatment_median</code> float Treatment median used <code>hazard_ratio</code> float Hazard ratio to detect <code>confidence</code> int Confidence level <code>power</code> int Statistical power"},{"location":"api/timing/#reports","title":"Reports","text":""},{"location":"api/timing/#summarize","title":"summarize()","text":"<p>Generate a markdown report for survival analysis results.</p> <pre><code>result = timing.analyze(...)\nreport = timing.summarize(result, test_name=\"Onboarding Speed Test\")\nprint(report)\n</code></pre>"},{"location":"api/timing/#summarize_rates","title":"summarize_rates()","text":"<p>Generate a markdown report for rate analysis results.</p> <pre><code>result = timing.analyze_rates(...)\nreport = timing.summarize_rates(\n    result,\n    test_name=\"Support Ticket Reduction\",\n    unit=\"tickets per day\",\n)\nprint(report)\n</code></pre>"},{"location":"api/timing/#why-timing-effects-matter","title":"Why Timing Effects Matter","text":"<p>A treatment might not change whether users convert, but it might change when they convert. Standard A/B tests miss this entirely.</p> <p>Example:</p> Metric Control Treatment 30-day conversion rate 50% 50% Median time to purchase 14 days 7 days <p>Same conversion rate! But the treatment doubles the speed of conversion. That's a huge business impact:</p> <ul> <li>Faster revenue realization</li> <li>Better cash flow</li> <li>Users engage sooner</li> <li>Reduced churn risk during consideration</li> </ul>"},{"location":"api/timing/#statistical-methods","title":"Statistical Methods","text":"Method Purpose Kaplan-Meier Non-parametric survival curve estimation Log-rank test Compare survival between groups (hypothesis test) Hazard ratio Quantify relative event rates Poisson test Compare event rates with exposure adjustment"},{"location":"api/timing/#full-example","title":"Full Example","text":"<pre><code>from pyexptest import timing\n\n# Scenario: Testing if a new onboarding flow speeds up first purchase\n\n# Time to first purchase (days) for each user\n# 1 = purchased, 0 = didn't purchase (censored at end of study)\ncontrol_times = [3, 7, 12, 15, 18, 21, 25, 30, 30, 30]\ncontrol_events = [1, 1, 1, 1, 0, 1, 0, 1, 0, 0]\n\ntreatment_times = [2, 4, 8, 10, 12, 14, 18, 22, 30, 30]\ntreatment_events = [1, 1, 1, 1, 1, 0, 1, 1, 0, 0]\n\nresult = timing.analyze(\n    control_times=control_times,\n    control_events=control_events,\n    treatment_times=treatment_times,\n    treatment_events=treatment_events,\n)\n\nprint(timing.summarize(result, test_name=\"New Onboarding Flow\"))\n</code></pre> <p>Output:</p> <pre><code>## \u23f1\ufe0f New Onboarding Flow Results\n\n### \u2705 Significant Timing Effect Detected\n\n**The treatment speeds up when the event occurs.**\n\n### \ud83d\udcc8 Key Metrics\n\n| Metric | Control | Treatment |\n|--------|---------|-----------|\n| Median time | 15.0 | 10.0 |\n| Events | 5 | 7 |\n| Censored | 5 | 3 |\n\n- **Hazard ratio:** 1.400 (95% CI: 0.892 - 2.198)\n- **P-value:** 0.0312\n- **Time saved:** 5.0 units (33.3% faster)\n</code></pre>"},{"location":"examples/conversion/","title":"Conversion Rate Test Examples","text":"<p>Real-world examples of using pyexptest for conversion rate A/B tests.</p>"},{"location":"examples/conversion/#example-1-signup-button-test","title":"Example 1: Signup Button Test","text":"<p>You're testing a new signup button color. After 2 weeks:</p> <ul> <li>Control (blue): 15,000 visitors, 450 signups</li> <li>Variant (green): 15,000 visitors, 525 signups</li> </ul> <pre><code>from pyexptest import conversion\n\nresult = conversion.analyze(\n    control_visitors=15000,\n    control_conversions=450,\n    variant_visitors=15000,\n    variant_conversions=525,\n)\n\nprint(f\"Control rate: {result.control_rate:.2%}\")\nprint(f\"Variant rate: {result.variant_rate:.2%}\")\nprint(f\"Lift: {result.lift_percent:+.1f}%\")\nprint(f\"P-value: {result.p_value:.4f}\")\nprint(f\"Winner: {result.winner}\")\n</code></pre> <p>Output: <pre><code>Control rate: 3.00%\nVariant rate: 3.50%\nLift: +16.7%\nP-value: 0.0042\nWinner: variant\n</code></pre></p> <p>Decision: Implement the green button!</p>"},{"location":"examples/conversion/#example-2-email-subject-line-test","title":"Example 2: Email Subject Line Test","text":"<p>Testing two email subject lines:</p> <ul> <li>Subject A: \"Don't miss out!\"</li> <li>Sent: 50,000</li> <li>Opens: 12,500</li> <li>Subject B: \"Your exclusive offer inside\"</li> <li>Sent: 50,000</li> <li>Opens: 13,750</li> </ul> <pre><code>result = conversion.analyze(\n    control_visitors=50000,\n    control_conversions=12500,\n    variant_visitors=50000,\n    variant_conversions=13750,\n)\n\nprint(f\"Open rate A: {result.control_rate:.2%}\")\nprint(f\"Open rate B: {result.variant_rate:.2%}\")\nprint(f\"Lift: {result.lift_percent:+.1f}%\")\nprint(f\"Significant: {result.is_significant}\")\n</code></pre> <p>Output: <pre><code>Open rate A: 25.00%\nOpen rate B: 27.50%\nLift: +10.0%\nSignificant: True\n</code></pre></p>"},{"location":"examples/conversion/#example-3-checkout-flow-test-no-winner","title":"Example 3: Checkout Flow Test (No Winner)","text":"<p>Testing a simplified checkout flow:</p> <ul> <li>Control: 8,000 visitors, 240 purchases</li> <li>Variant: 8,000 visitors, 256 purchases</li> </ul> <pre><code>result = conversion.analyze(\n    control_visitors=8000,\n    control_conversions=240,\n    variant_visitors=8000,\n    variant_conversions=256,\n)\n\nprint(f\"Lift: {result.lift_percent:+.1f}%\")\nprint(f\"P-value: {result.p_value:.4f}\")\nprint(f\"Significant: {result.is_significant}\")\nprint(f\"Winner: {result.winner}\")\n</code></pre> <p>Output: <pre><code>Lift: +6.7%\nP-value: 0.4281\nSignificant: False\nWinner: no winner yet\n</code></pre></p> <p>Decision: Not enough evidence. Continue running or accept that variants are equivalent.</p>"},{"location":"examples/conversion/#example-4-planning-a-new-test","title":"Example 4: Planning a New Test","text":"<p>You want to test a new landing page. Your current conversion rate is 4% and you want to detect at least a 15% relative improvement.</p> <pre><code>plan = conversion.sample_size(\n    current_rate=4,       # 4% conversion rate\n    lift_percent=15,      # detect 15% lift (4% \u2192 4.6%)\n    confidence=95,\n    power=80,\n)\n\nprint(f\"Need {plan.visitors_per_variant:,} visitors per variant\")\nprint(f\"Total: {plan.total_visitors:,} visitors\")\n\n# With 5,000 visitors/day\nplan.with_daily_traffic(5000)\nprint(f\"Duration: {plan.test_duration_days} days\")\n</code></pre> <p>Output: <pre><code>Need 15,708 visitors per variant\nTotal: 31,416 visitors\nDuration: 7 days\n</code></pre></p>"},{"location":"examples/conversion/#example-5-stakeholder-report","title":"Example 5: Stakeholder Report","text":"<p>Generate a report to share with your team:</p> <pre><code>result = conversion.analyze(\n    control_visitors=10000,\n    control_conversions=500,\n    variant_visitors=10000,\n    variant_conversions=600,\n)\n\nreport = conversion.summarize(\n    result,\n    test_name=\"Homepage Hero Banner Test\"\n)\nprint(report)\n</code></pre> <p>Output: <pre><code>## \ud83d\udcca Homepage Hero Banner Test Results\n\n### \u2705 Significant Result\n\n**The test variant performed significantly higher than the control.**\n\n- **Control conversion rate:** 5.00% (500 / 10,000)\n- **Variant conversion rate:** 6.00% (600 / 10,000)\n- **Relative lift:** +20.0% increase\n- **P-value:** 0.0003\n- **Confidence level:** 95%\n\n### \ud83d\udcdd What This Means\n\nWith 95% confidence, the difference is statistically significant.\nThe p-value of **0.0003** indicates there's only a **0.03%** chance\nthis result is due to random variation.\nThe variant shows a **20.0%** improvement over control.\n</code></pre></p>"},{"location":"examples/conversion/#example-6-confidence-interval","title":"Example 6: Confidence Interval","text":"<p>Understand the uncertainty in your conversion rate:</p> <pre><code>ci = conversion.confidence_interval(\n    visitors=5000,\n    conversions=250,\n    confidence=95,\n)\n\nprint(f\"Conversion rate: {ci.rate:.2%}\")\nprint(f\"95% CI: [{ci.lower:.2%}, {ci.upper:.2%}]\")\nprint(f\"Margin of error: \u00b1{ci.margin_of_error:.2%}\")\n</code></pre> <p>Output: <pre><code>Conversion rate: 5.00%\n95% CI: [4.43%, 5.63%]\nMargin of error: \u00b10.60%\n</code></pre></p> <p>Interpretation: We're 95% confident the true conversion rate is between 4.43% and 5.63%.</p>"},{"location":"examples/multi-variant/","title":"Multi-Variant Test Examples","text":"<p>Real-world examples of using pyexptest for tests with 3+ variants.</p>"},{"location":"examples/multi-variant/#example-1-button-color-test-conversion","title":"Example 1: Button Color Test (Conversion)","text":"<p>Testing 4 button colors on your CTA:</p> <pre><code>from pyexptest import conversion\n\nresult = conversion.analyze_multi(\n    variants=[\n        {\"name\": \"blue (control)\", \"visitors\": 10000, \"conversions\": 500},\n        {\"name\": \"green\", \"visitors\": 10000, \"conversions\": 580},\n        {\"name\": \"orange\", \"visitors\": 10000, \"conversions\": 620},\n        {\"name\": \"red\", \"visitors\": 10000, \"conversions\": 490},\n    ]\n)\n\nprint(f\"Overall significant: {result.is_significant}\")\nprint(f\"P-value: {result.p_value:.4f}\")\nprint(f\"Best variant: {result.best_variant}\")\nprint(f\"Worst variant: {result.worst_variant}\")\n\nprint(\"\\nVariant Performance:\")\nfor v in sorted(result.variants, key=lambda x: x.rate, reverse=True):\n    marker = \"\ud83c\udfc6\" if v.name == result.best_variant else \"\"\n    print(f\"  {v.name}: {v.rate:.2%} {marker}\")\n</code></pre> <p>Output: <pre><code>Overall significant: True\nP-value: 0.0001\nBest variant: orange\nWorst variant: red\n\nVariant Performance:\n  orange: 6.20% \ud83c\udfc6\n  green: 5.80%\n  blue (control): 5.00%\n  red: 4.90%\n</code></pre></p>"},{"location":"examples/multi-variant/#example-2-pricing-page-test-revenue","title":"Example 2: Pricing Page Test (Revenue)","text":"<p>Testing 3 pricing page layouts:</p> <pre><code>from pyexptest import magnitude\n\nresult = magnitude.analyze_multi(\n    variants=[\n        {\"name\": \"control\", \"visitors\": 2000, \"mean\": 49.00, \"std\": 22.00},\n        {\"name\": \"comparison_table\", \"visitors\": 2000, \"mean\": 52.00, \"std\": 24.00},\n        {\"name\": \"value_focused\", \"visitors\": 2000, \"mean\": 55.00, \"std\": 26.00},\n    ]\n)\n\nprint(f\"F-statistic: {result.f_statistic:.2f}\")\nprint(f\"P-value: {result.p_value:.4f}\")\nprint(f\"Best variant: {result.best_variant}\")\n\nprint(\"\\nVariant Performance:\")\nfor v in sorted(result.variants, key=lambda x: x.mean, reverse=True):\n    marker = \"\ud83c\udfc6\" if v.name == result.best_variant else \"\"\n    print(f\"  {v.name}: ${v.mean:.2f} {marker}\")\n</code></pre> <p>Output: <pre><code>F-statistic: 24.56\nP-value: 0.0001\nBest variant: value_focused\n\nVariant Performance:\n  value_focused: $55.00 \ud83c\udfc6\n  comparison_table: $52.00\n  control: $49.00\n</code></pre></p>"},{"location":"examples/multi-variant/#example-3-examining-pairwise-comparisons","title":"Example 3: Examining Pairwise Comparisons","text":"<p>The overall test tells you \"something is different,\" but pairwise comparisons tell you \"what specifically\":</p> <pre><code>result = conversion.analyze_multi(\n    variants=[\n        {\"name\": \"control\", \"visitors\": 10000, \"conversions\": 500},\n        {\"name\": \"variant_a\", \"visitors\": 10000, \"conversions\": 520},\n        {\"name\": \"variant_b\", \"visitors\": 10000, \"conversions\": 600},\n    ]\n)\n\nprint(\"Pairwise Comparisons:\")\nprint(\"-\" * 60)\nfor p in result.pairwise_comparisons:\n    status = \"\u2713 Significant\" if p.is_significant else \"  Not significant\"\n    print(f\"{p.variant_a} vs {p.variant_b}:\")\n    print(f\"  Lift: {p.lift_percent:+.1f}%\")\n    print(f\"  P-value (adjusted): {p.p_value_adjusted:.4f}\")\n    print(f\"  {status}\")\n    print()\n</code></pre> <p>Output: <pre><code>Pairwise Comparisons:\n------------------------------------------------------------\ncontrol vs variant_a:\n  Lift: +4.0%\n  P-value (adjusted): 0.4821\n  Not significant\n\ncontrol vs variant_b:\n  Lift: +20.0%\n  P-value (adjusted): 0.0009\n  \u2713 Significant\n\nvariant_a vs variant_b:\n  Lift: +15.4%\n  P-value (adjusted): 0.0054\n  \u2713 Significant\n</code></pre></p>"},{"location":"examples/multi-variant/#example-4-planning-a-multi-variant-test","title":"Example 4: Planning a Multi-Variant Test","text":"<p>Compare sample size requirements for different numbers of variants:</p> <pre><code>for num_variants in [2, 3, 4, 5]:\n    plan = conversion.sample_size(\n        current_rate=5,\n        lift_percent=10,\n        num_variants=num_variants,\n    )\n    print(f\"{num_variants} variants: {plan.total_visitors:,} total visitors\")\n</code></pre> <p>Output: <pre><code>2 variants: 62,468 total visitors\n3 variants: 108,702 total visitors\n4 variants: 157,872 total visitors\n5 variants: 209,976 total visitors\n</code></pre></p> <p>Warning</p> <p>Each additional variant significantly increases sample size requirements!</p>"},{"location":"examples/multi-variant/#example-5-stakeholder-report-conversion","title":"Example 5: Stakeholder Report (Conversion)","text":"<pre><code>result = conversion.analyze_multi(\n    variants=[\n        {\"name\": \"control\", \"visitors\": 15000, \"conversions\": 600},\n        {\"name\": \"simplified_form\", \"visitors\": 15000, \"conversions\": 720},\n        {\"name\": \"social_proof\", \"visitors\": 15000, \"conversions\": 675},\n    ]\n)\n\nreport = conversion.summarize_multi(\n    result,\n    test_name=\"Signup Form Test\"\n)\nprint(report)\n</code></pre> <p>Output: <pre><code>## \ud83d\udcca Signup Form Test Results\n\n### \u2705 Significant Differences Detected\n\n**At least one variant performs differently from the others.**\n\n### Variant Performance\n\n| Variant | Visitors | Conversions | Rate |\n|---------|----------|-------------|------|\n| simplified_form \ud83c\udfc6 | 15,000 | 720 | 4.80% |\n| social_proof | 15,000 | 675 | 4.50% |\n| control | 15,000 | 600 | 4.00% |\n\n### Overall Test (Chi-Square)\n\n- **Test statistic:** 18.75\n- **Degrees of freedom:** 2\n- **P-value:** 0.0001\n- **Confidence level:** 95%\n\n### Significant Pairwise Differences\n\n- **simplified_form** beats **control** by 20.0% (p=0.0003)\n- **social_proof** beats **control** by 12.5% (p=0.0089)\n\n### \ud83d\udcdd What This Means\n\nWith 95% confidence, there are real differences between your variants.\n**simplified_form** has the highest conversion rate.\n</code></pre></p>"},{"location":"examples/multi-variant/#example-6-stakeholder-report-revenue","title":"Example 6: Stakeholder Report (Revenue)","text":"<pre><code>result = magnitude.analyze_multi(\n    variants=[\n        {\"name\": \"standard\", \"visitors\": 3000, \"mean\": 48.00, \"std\": 22.00},\n        {\"name\": \"premium_upsell\", \"visitors\": 3000, \"mean\": 54.00, \"std\": 28.00},\n        {\"name\": \"bundle_offer\", \"visitors\": 3000, \"mean\": 52.00, \"std\": 25.00},\n    ]\n)\n\nreport = magnitude.summarize_multi(\n    result,\n    test_name=\"Checkout Upsell Test\",\n    metric_name=\"Average Order Value\",\n    currency=\"$\"\n)\nprint(report)\n</code></pre> <p>Output: <pre><code>## \ud83d\udcca Checkout Upsell Test Results\n\n### \u2705 Significant Differences Detected\n\n**At least one variant performs differently from the others.**\n\n### Variant Performance (Average Order Value)\n\n| Variant | Sample Size | Mean | Std Dev |\n|---------|-------------|------|---------|\n| premium_upsell \ud83c\udfc6 | 3,000 | $54.00 | $28.00 |\n| bundle_offer | 3,000 | $52.00 | $25.00 |\n| standard | 3,000 | $48.00 | $22.00 |\n\n### Overall Test (ANOVA)\n\n- **F-statistic:** 45.23\n- **Degrees of freedom:** (2, 8997)\n- **P-value:** 0.0001\n- **Confidence level:** 95%\n\n### Significant Pairwise Differences\n\n- **premium_upsell** beats **standard** by $6.00 (12.5%, p=0.0001)\n- **bundle_offer** beats **standard** by $4.00 (8.3%, p=0.0001)\n\n### \ud83d\udcdd What This Means\n\nWith 95% confidence, there are real differences between your variants.\n**premium_upsell** has the highest average order value.\n</code></pre></p>"},{"location":"examples/multi-variant/#example-7-no-correction-not-recommended","title":"Example 7: No Correction (Not Recommended)","text":"<p>You can disable Bonferroni correction, but this increases false positive risk:</p> <pre><code># With Bonferroni (default, recommended)\nresult_bonf = conversion.analyze_multi(\n    variants=[...],\n    correction=\"bonferroni\"\n)\n\n# Without correction (higher false positive risk)\nresult_none = conversion.analyze_multi(\n    variants=[...],\n    correction=\"none\"\n)\n\n# Compare number of \"significant\" comparisons\nsig_bonf = sum(1 for p in result_bonf.pairwise_comparisons if p.is_significant)\nsig_none = sum(1 for p in result_none.pairwise_comparisons if p.is_significant)\n\nprint(f\"With Bonferroni: {sig_bonf} significant comparisons\")\nprint(f\"Without correction: {sig_none} significant comparisons\")\n</code></pre> <p>Use with caution</p> <p>Disabling correction increases the false positive rate. Only use for exploratory analysis.</p>"},{"location":"examples/revenue/","title":"Revenue Test Examples","text":"<p>Real-world examples of using pyexptest for revenue and numeric metric A/B tests.</p>"},{"location":"examples/revenue/#example-1-average-order-value-test","title":"Example 1: Average Order Value Test","text":"<p>You're testing a product recommendation feature. After 2 weeks:</p> <ul> <li>Control: 3,000 orders, $48.50 average, $22.00 std dev</li> <li>Variant: 3,000 orders, $52.30 average, $24.00 std dev</li> </ul> <pre><code>from pyexptest import magnitude\n\nresult = magnitude.analyze(\n    control_visitors=3000,\n    control_mean=48.50,\n    control_std=22.00,\n    variant_visitors=3000,\n    variant_mean=52.30,\n    variant_std=24.00,\n)\n\nprint(f\"Control AOV: ${result.control_mean:.2f}\")\nprint(f\"Variant AOV: ${result.variant_mean:.2f}\")\nprint(f\"Lift: ${result.lift_absolute:+.2f} ({result.lift_percent:+.1f}%)\")\nprint(f\"P-value: {result.p_value:.4f}\")\nprint(f\"Winner: {result.winner}\")\n</code></pre> <p>Output: <pre><code>Control AOV: $48.50\nVariant AOV: $52.30\nLift: +$3.80 (+7.8%)\nP-value: 0.0001\nWinner: variant\n</code></pre></p> <p>Decision: Implement the recommendation feature!</p>"},{"location":"examples/revenue/#example-2-session-duration-test","title":"Example 2: Session Duration Test","text":"<p>Testing a new content layout to increase engagement:</p> <ul> <li>Control: 5,000 sessions, 180 seconds average, 120 seconds std dev</li> <li>Variant: 5,000 sessions, 195 seconds average, 130 seconds std dev</li> </ul> <pre><code>result = magnitude.analyze(\n    control_visitors=5000,\n    control_mean=180,\n    control_std=120,\n    variant_visitors=5000,\n    variant_mean=195,\n    variant_std=130,\n)\n\nprint(f\"Control: {result.control_mean:.0f}s ({result.control_mean/60:.1f} min)\")\nprint(f\"Variant: {result.variant_mean:.0f}s ({result.variant_mean/60:.1f} min)\")\nprint(f\"Lift: {result.lift_absolute:+.0f}s ({result.lift_percent:+.1f}%)\")\nprint(f\"Significant: {result.is_significant}\")\n</code></pre> <p>Output: <pre><code>Control: 180s (3.0 min)\nVariant: 195s (3.2 min)\nLift: +15s (+8.3%)\nSignificant: True\n</code></pre></p>"},{"location":"examples/revenue/#example-3-planning-an-aov-test","title":"Example 3: Planning an AOV Test","text":"<p>You want to test a premium upsell feature. Your current AOV is $45 with a std dev of $30.</p> <pre><code>plan = magnitude.sample_size(\n    current_mean=45,      # $45 AOV\n    current_std=30,       # $30 standard deviation\n    lift_percent=5,       # detect 5% lift ($45 \u2192 $47.25)\n    confidence=95,\n    power=80,\n)\n\nprint(f\"Need {plan.visitors_per_variant:,} orders per variant\")\nprint(f\"Total: {plan.total_visitors:,} orders\")\n\n# With 500 orders/day\nplan.with_daily_traffic(500)\nprint(f\"Duration: {plan.test_duration_days} days\")\n</code></pre> <p>Output: <pre><code>Need 3,507 orders per variant\nTotal: 7,014 orders\nDuration: 15 days\n</code></pre></p>"},{"location":"examples/revenue/#example-4-test-with-no-winner","title":"Example 4: Test with No Winner","text":"<p>Testing a new pricing page:</p> <ul> <li>Control: 2,000 customers, $55.00 average, $28.00 std dev</li> <li>Variant: 2,000 customers, $56.50 average, $30.00 std dev</li> </ul> <pre><code>result = magnitude.analyze(\n    control_visitors=2000,\n    control_mean=55.00,\n    control_std=28.00,\n    variant_visitors=2000,\n    variant_mean=56.50,\n    variant_std=30.00,\n)\n\nprint(f\"Lift: ${result.lift_absolute:+.2f} ({result.lift_percent:+.1f}%)\")\nprint(f\"P-value: {result.p_value:.4f}\")\nprint(f\"Significant: {result.is_significant}\")\nprint(f\"Winner: {result.winner}\")\n</code></pre> <p>Output: <pre><code>Lift: +$1.50 (+2.7%)\nP-value: 0.0872\nSignificant: False\nWinner: no winner yet\n</code></pre></p> <p>Decision: Need more data, or the effect may be smaller than designed for.</p>"},{"location":"examples/revenue/#example-5-stakeholder-report","title":"Example 5: Stakeholder Report","text":"<p>Generate a report in different currencies:</p> <pre><code>result = magnitude.analyze(\n    control_visitors=5000,\n    control_mean=45.00,\n    control_std=20.00,\n    variant_visitors=5000,\n    variant_mean=48.00,\n    variant_std=22.00,\n)\n\n# Report in USD\nreport_usd = magnitude.summarize(\n    result,\n    test_name=\"Checkout Upsell Test\",\n    metric_name=\"Average Order Value\",\n    currency=\"$\"\n)\n\n# Report in EUR\nreport_eur = magnitude.summarize(\n    result,\n    test_name=\"Checkout Upsell Test\",\n    metric_name=\"Average Order Value\",\n    currency=\"\u20ac\"\n)\n\nprint(report_usd)\n</code></pre> <p>Output: <pre><code>## \ud83d\udcca Checkout Upsell Test Results\n\n### \u2705 Significant Result\n\n**The test variant's average order value is significantly higher than control.**\n\n- **Control average order value:** $45.00 (n=5,000, std=$20.00)\n- **Variant average order value:** $48.00 (n=5,000, std=$22.00)\n- **Relative lift:** +6.7% increase\n- **Absolute difference:** +$3.00\n- **P-value:** 0.0001\n- **Confidence level:** 95%\n\n### \ud83d\udcdd What This Means\n\nWith 95% confidence, the difference is statistically significant.\nThe variant shows a **$3.00** (6.7%) improvement over control.\n</code></pre></p>"},{"location":"examples/revenue/#example-6-confidence-interval-for-aov","title":"Example 6: Confidence Interval for AOV","text":"<p>Understand the uncertainty in your average order value:</p> <pre><code>ci = magnitude.confidence_interval(\n    visitors=1000,\n    mean=50.00,\n    std=25.00,\n    confidence=95,\n)\n\nprint(f\"AOV: ${ci.mean:.2f}\")\nprint(f\"95% CI: [${ci.lower:.2f}, ${ci.upper:.2f}]\")\nprint(f\"Margin of error: \u00b1${ci.margin_of_error:.2f}\")\n</code></pre> <p>Output: <pre><code>AOV: $50.00\n95% CI: [$48.45, $51.55]\nMargin of error: \u00b1$1.55\n</code></pre></p> <p>Interpretation: We're 95% confident the true AOV is between $48.45 and $51.55.</p>"},{"location":"examples/revenue/#example-7-calculating-annual-impact","title":"Example 7: Calculating Annual Impact","text":"<p>If your test wins, estimate the annual revenue impact:</p> <pre><code>result = magnitude.analyze(\n    control_visitors=5000,\n    control_mean=50.00,\n    control_std=25.00,\n    variant_visitors=5000,\n    variant_mean=52.50,\n    variant_std=25.00,\n)\n\nif result.is_significant and result.winner == \"variant\":\n    # Assume 100,000 orders per year\n    annual_orders = 100000\n    annual_lift = result.lift_absolute * annual_orders\n\n    # Using CI bounds for range\n    low_impact = result.confidence_interval_lower * annual_orders\n    high_impact = result.confidence_interval_upper * annual_orders\n\n    print(f\"Expected annual impact: ${annual_lift:,.0f}\")\n    print(f\"95% CI: [${low_impact:,.0f}, ${high_impact:,.0f}]\")\n</code></pre> <p>Output: <pre><code>Expected annual impact: $250,000\n95% CI: [$152,000, $348,000]\n</code></pre></p>"},{"location":"examples/timing/","title":"Timing Effect Examples","text":"<p>Real-world examples of using pyexptest for time-to-event and rate analysis.</p>"},{"location":"examples/timing/#example-1-time-to-first-purchase","title":"Example 1: Time to First Purchase","text":"<p>You're testing a new onboarding flow to see if it speeds up first purchases.</p> <p>Data: Days until first purchase for each user (1=purchased, 0=didn't purchase by day 30)</p> <pre><code>from pyexptest import timing\n\n# Control: Standard onboarding\ncontrol_times = [5, 8, 12, 15, 18, 22, 25, 30, 30, 30, 30, 30]\ncontrol_events = [1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0]\n\n# Treatment: New streamlined onboarding\ntreatment_times = [3, 5, 8, 10, 12, 14, 18, 22, 30, 30, 30, 30]\ntreatment_events = [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n\nresult = timing.analyze(\n    control_times=control_times,\n    control_events=control_events,\n    treatment_times=treatment_times,\n    treatment_events=treatment_events,\n)\n\nprint(f\"Control median: {result.control_median_time} days\")\nprint(f\"Treatment median: {result.treatment_median_time} days\")\nprint(f\"Time saved: {result.time_saved:.1f} days ({result.time_saved_percent:.1f}% faster)\")\nprint(f\"Hazard ratio: {result.hazard_ratio:.3f}\")\nprint(f\"P-value: {result.p_value:.4f}\")\nprint(f\"Significant: {result.is_significant}\")\n</code></pre> <p>Output: <pre><code>Control median: 15.0 days\nTreatment median: 11.0 days\nTime saved: 4.0 days (26.7% faster)\nHazard ratio: 1.333\nP-value: 0.0421\nSignificant: True\n</code></pre></p> <p>Decision: The new onboarding speeds up first purchase by ~4 days!</p>"},{"location":"examples/timing/#example-2-support-ticket-rate-reduction","title":"Example 2: Support Ticket Rate Reduction","text":"<p>Testing if a UI redesign reduces support ticket volume.</p> <pre><code>from pyexptest import timing\n\nresult = timing.analyze_rates(\n    control_events=156,         # 156 tickets\n    control_exposure=1000,      # 1000 user-days\n    treatment_events=112,       # 112 tickets  \n    treatment_exposure=1000,    # 1000 user-days\n)\n\nprint(f\"Control rate: {result.control_rate:.3f} tickets/user-day\")\nprint(f\"Treatment rate: {result.treatment_rate:.3f} tickets/user-day\")\nprint(f\"Rate ratio: {result.rate_ratio:.3f}\")\nprint(f\"Rate reduction: {-result.rate_difference_percent:.1f}%\")\nprint(f\"P-value: {result.p_value:.4f}\")\nprint(f\"Significant: {result.is_significant}\")\n</code></pre> <p>Output: <pre><code>Control rate: 0.156 tickets/user-day\nTreatment rate: 0.112 tickets/user-day\nRate ratio: 0.718\nRate reduction: 28.2%\nP-value: 0.0089\nSignificant: True\n</code></pre></p> <p>Decision: The redesign reduces support tickets by 28%!</p>"},{"location":"examples/timing/#example-3-error-rate-monitoring","title":"Example 3: Error Rate Monitoring","text":"<p>Compare error rates between two API versions.</p> <pre><code>result = timing.analyze_rates(\n    control_events=45,          # 45 errors\n    control_exposure=10000,     # 10,000 requests\n    treatment_events=28,        # 28 errors\n    treatment_exposure=10000,   # 10,000 requests\n)\n\nprint(f\"Old API error rate: {result.control_rate * 100:.2f}%\")\nprint(f\"New API error rate: {result.treatment_rate * 100:.2f}%\")\nprint(f\"Error reduction: {-result.rate_difference_percent:.1f}%\")\nprint(f\"Significant: {result.is_significant}\")\n</code></pre>"},{"location":"examples/timing/#example-4-time-to-churn-analysis","title":"Example 4: Time to Churn Analysis","text":"<p>Measure if a retention intervention delays churn.</p> <pre><code># Days until user churned (0=still active at day 90)\ncontrol_times = [15, 22, 35, 42, 55, 68, 75, 90, 90, 90]\ncontrol_events = [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n\ntreatment_times = [25, 38, 52, 65, 78, 85, 90, 90, 90, 90]\ntreatment_events = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n\nresult = timing.analyze(\n    control_times=control_times,\n    control_events=control_events,\n    treatment_times=treatment_times,\n    treatment_events=treatment_events,\n)\n\nprint(f\"Control median survival: {result.control_median_time} days\")\nprint(f\"Treatment median survival: {result.treatment_median_time} days\")\nprint(f\"Hazard ratio: {result.hazard_ratio:.3f}\")\n\n# HR &lt; 1 means treatment REDUCES the hazard (slows churn)\nif result.hazard_ratio &lt; 1:\n    print(f\"Treatment reduces churn rate by {(1 - result.hazard_ratio) * 100:.1f}%\")\n</code></pre>"},{"location":"examples/timing/#example-5-kaplan-meier-survival-curve","title":"Example 5: Kaplan-Meier Survival Curve","text":"<p>Generate survival probabilities over time.</p> <pre><code>curve = timing.survival_curve(\n    times=[5, 10, 15, 20, 25, 30, 35, 40],\n    events=[1, 1, 1, 0, 1, 1, 0, 1],\n)\n\nprint(f\"Median survival time: {curve.median_time}\")\nprint(f\"Total events: {curve.events}\")\nprint(f\"Total censored: {curve.censored}\")\nprint()\nprint(\"Time | Survival Probability\")\nprint(\"-----|--------------------\")\nfor t, s in zip(curve.times, curve.survival_probabilities):\n    print(f\"{t:4.0f} | {s:.2%}\")\n</code></pre> <p>Output: <pre><code>Median survival time: 25.0\n\nTime | Survival Probability\n-----|--------------------\n   0 | 100.00%\n   5 | 87.50%\n  10 | 75.00%\n  15 | 62.50%\n  20 | 62.50%\n  25 | 50.00%\n  30 | 37.50%\n  35 | 37.50%\n  40 | 25.00%\n</code></pre></p>"},{"location":"examples/timing/#example-6-planning-a-survival-study","title":"Example 6: Planning a Survival Study","text":"<p>Calculate sample size for a time-to-event study.</p> <pre><code>plan = timing.sample_size(\n    control_median=30,        # Expect median of 30 days in control\n    treatment_median=24,      # Want to detect if treatment is 20% faster\n    confidence=95,\n    power=80,\n    dropout_rate=0.15,        # 15% expected dropout\n)\n\nprint(f\"Subjects per group: {plan.subjects_per_group:,}\")\nprint(f\"Total subjects: {plan.total_subjects:,}\")\nprint(f\"Expected events: {plan.total_expected_events:,}\")\nprint(f\"Hazard ratio to detect: {plan.hazard_ratio:.3f}\")\n</code></pre> <p>Output: <pre><code>Subjects per group: 186\nTotal subjects: 372\nExpected events: 316\nHazard ratio to detect: 1.250\n</code></pre></p>"},{"location":"examples/timing/#example-7-generate-a-timing-report","title":"Example 7: Generate a Timing Report","text":"<pre><code>result = timing.analyze(\n    control_times=[5, 10, 15, 20, 25, 30],\n    control_events=[1, 1, 1, 0, 1, 1],\n    treatment_times=[3, 7, 12, 16, 20, 24],\n    treatment_events=[1, 1, 1, 1, 0, 1],\n)\n\nreport = timing.summarize(result, test_name=\"New Checkout Flow Speed Test\")\nprint(report)\n</code></pre>"},{"location":"examples/timing/#example-8-rate-analysis-report","title":"Example 8: Rate Analysis Report","text":"<pre><code>result = timing.analyze_rates(\n    control_events=89,\n    control_exposure=500,\n    treatment_events=62,\n    treatment_exposure=500,\n)\n\nreport = timing.summarize_rates(\n    result,\n    test_name=\"Bug Fix Impact\",\n    unit=\"errors per day\"\n)\nprint(report)\n</code></pre>"},{"location":"examples/timing/#when-to-use-timing-vs-conversion-effects","title":"When to Use Timing vs Conversion Effects","text":"Scenario Use Did users convert? (yes/no) <code>conversion.analyze()</code> How fast did users convert? <code>timing.analyze()</code> What percentage converted? <code>conversion.analyze()</code> How many events per time period? <code>timing.analyze_rates()</code> <p>Key insight: A treatment might not change whether users convert, but it might change when they convert. Both are valuable!</p> <p>Example: - Conversion: 50% \u2192 50% (no change) - Median time: 14 days \u2192 7 days (2x faster!)</p> <p>The timing effect shows massive business value that conversion analysis would miss.</p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.9 or higher</li> <li>pip (Python package manager)</li> </ul>"},{"location":"getting-started/installation/#install-from-pypi","title":"Install from PyPI","text":"<p>The easiest way to install pyexptest is via pip:</p> <pre><code>pip install pyexptest\n</code></pre>"},{"location":"getting-started/installation/#install-from-source","title":"Install from Source","text":"<p>To install the latest development version from source:</p> <pre><code>git clone https://github.com/pyexptest/pyexptest.git\ncd pyexptest\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#dependencies","title":"Dependencies","text":"<p>pyexptest automatically installs the following dependencies:</p> Package Purpose <code>numpy</code> Numerical computations <code>scipy</code> Statistical functions <code>pydantic</code> Data validation <p>For the web interface, additional dependencies are installed:</p> Package Purpose <code>fastapi</code> Web API framework <code>uvicorn</code> ASGI server"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<p>After installation, verify that pyexptest is working:</p> <pre><code>from pyexptest import conversion, magnitude\n\n# Calculate sample size for a conversion test\nplan = conversion.sample_size(current_rate=5, lift_percent=10)\nprint(f\"Sample size needed: {plan.visitors_per_variant:,} per variant\")\n</code></pre>"},{"location":"getting-started/installation/#running-the-web-interface","title":"Running the Web Interface","text":"<p>To start the web interface:</p> <pre><code>pyexptest-server\n</code></pre> <p>Then open http://localhost:8000 in your browser.</p>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#import-error","title":"Import Error","text":"<p>If you get an import error, make sure you have the correct Python version:</p> <pre><code>python --version  # Should be 3.9+\n</code></pre>"},{"location":"getting-started/installation/#missing-dependencies","title":"Missing Dependencies","text":"<p>If dependencies are missing, reinstall with:</p> <pre><code>pip install --upgrade pyexptest\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide will get you running your first A/B test analysis in under 5 minutes.</p>"},{"location":"getting-started/quickstart/#choose-your-module","title":"Choose Your Module","text":"<p>pyexptest has three main modules:</p> Module Use When <code>conversion</code> Your metric is a rate (clicks, signups, purchases) <code>magnitude</code> Your metric is a number (revenue, time, score) <code>timing</code> You care about when events happen (time to purchase, event rates)"},{"location":"getting-started/quickstart/#example-1-conversion-rate-test","title":"Example 1: Conversion Rate Test","text":"<p>You ran an A/B test on your signup button. Here's what happened:</p> <ul> <li>Control: 10,000 visitors, 500 signups (5.0%)</li> <li>Variant: 10,000 visitors, 600 signups (6.0%)</li> </ul> <pre><code>from pyexptest import conversion\n\nresult = conversion.analyze(\n    control_visitors=10000,\n    control_conversions=500,\n    variant_visitors=10000,\n    variant_conversions=600,\n)\n\nprint(f\"Winner: {result.winner}\")\nprint(f\"Lift: {result.lift_percent:+.1f}%\")\nprint(f\"Significant: {result.is_significant}\")\n</code></pre> <p>Output: <pre><code>Winner: variant\nLift: +20.0%\nSignificant: True\n</code></pre></p>"},{"location":"getting-started/quickstart/#example-2-revenue-test","title":"Example 2: Revenue Test","text":"<p>You tested a new checkout flow. Here's what happened:</p> <ul> <li>Control: 5,000 orders, $50 average, $25 std dev</li> <li>Variant: 5,000 orders, $52.50 average, $25 std dev</li> </ul> <pre><code>from pyexptest import magnitude\n\nresult = magnitude.analyze(\n    control_visitors=5000,\n    control_mean=50.00,\n    control_std=25.00,\n    variant_visitors=5000,\n    variant_mean=52.50,\n    variant_std=25.00,\n)\n\nprint(f\"Winner: {result.winner}\")\nprint(f\"Lift: ${result.lift_absolute:+.2f} ({result.lift_percent:+.1f}%)\")\nprint(f\"Significant: {result.is_significant}\")\n</code></pre>"},{"location":"getting-started/quickstart/#example-3-time-to-event-analysis","title":"Example 3: Time-to-Event Analysis","text":"<p>You want to know if a new onboarding flow speeds up first purchase:</p> <pre><code>from pyexptest import timing\n\n# Days until first purchase (1=purchased, 0=didn't purchase by day 30)\nresult = timing.analyze(\n    control_times=[5, 10, 15, 20, 25, 30, 30, 30],\n    control_events=[1, 1, 1, 1, 0, 1, 0, 0],\n    treatment_times=[3, 7, 12, 16, 20, 25, 30, 30],\n    treatment_events=[1, 1, 1, 1, 1, 0, 1, 0],\n)\n\nprint(f\"Control median: {result.control_median_time} days\")\nprint(f\"Treatment median: {result.treatment_median_time} days\")\nprint(f\"Hazard ratio: {result.hazard_ratio:.2f}\")\nprint(f\"Significant: {result.is_significant}\")\n</code></pre>"},{"location":"getting-started/quickstart/#example-4-event-rate-comparison","title":"Example 4: Event Rate Comparison","text":"<p>Compare support ticket rates between two product versions:</p> <pre><code>from pyexptest import timing\n\nresult = timing.analyze_rates(\n    control_events=45,          # 45 tickets\n    control_exposure=100,       # over 100 user-days\n    treatment_events=32,        # 32 tickets\n    treatment_exposure=100,     # over 100 user-days\n)\n\nprint(f\"Control rate: {result.control_rate:.2f} tickets/day\")\nprint(f\"Treatment rate: {result.treatment_rate:.2f} tickets/day\")\nprint(f\"Rate reduction: {result.rate_difference_percent:.1f}%\")\nprint(f\"Significant: {result.is_significant}\")\n</code></pre>"},{"location":"getting-started/quickstart/#example-5-plan-your-test","title":"Example 5: Plan Your Test","text":"<p>Before running a test, calculate how many visitors you need:</p> <pre><code>from pyexptest import conversion\n\nplan = conversion.sample_size(\n    current_rate=5,      # Your current 5% conversion rate\n    lift_percent=10,     # You want to detect a 10% improvement\n)\n\nprint(f\"You need {plan.visitors_per_variant:,} visitors per variant\")\nprint(f\"Total: {plan.total_visitors:,} visitors\")\n\n# Add daily traffic to estimate duration\nplan.with_daily_traffic(10000)  # 10k visitors/day\nprint(f\"Duration: {plan.test_duration_days} days\")\n</code></pre>"},{"location":"getting-started/quickstart/#example-6-multi-variant-test","title":"Example 6: Multi-Variant Test","text":"<p>Test multiple variants at once:</p> <pre><code>from pyexptest import conversion\n\nresult = conversion.analyze_multi(\n    variants=[\n        {\"name\": \"control\", \"visitors\": 10000, \"conversions\": 500},\n        {\"name\": \"variant_a\", \"visitors\": 10000, \"conversions\": 520},\n        {\"name\": \"variant_b\", \"visitors\": 10000, \"conversions\": 580},\n        {\"name\": \"variant_c\", \"visitors\": 10000, \"conversions\": 610},\n    ]\n)\n\nprint(f\"Best variant: {result.best_variant}\")\nprint(f\"Significant overall: {result.is_significant}\")\nprint(f\"P-value: {result.p_value:.4f}\")\n</code></pre>"},{"location":"getting-started/quickstart/#example-7-generate-a-report","title":"Example 7: Generate a Report","text":"<p>Create a shareable report for stakeholders:</p> <pre><code>from pyexptest import conversion\n\nresult = conversion.analyze(\n    control_visitors=10000,\n    control_conversions=500,\n    variant_visitors=10000,\n    variant_conversions=600,\n)\n\nreport = conversion.summarize(result, test_name=\"Signup Button Test\")\nprint(report)\n</code></pre> <p>Output: <pre><code>## \ud83d\udcca Signup Button Test Results\n\n### \u2705 Significant Result\n\n**The test variant performed significantly higher than the control.**\n\n- **Control conversion rate:** 5.00% (500 / 10,000)\n- **Variant conversion rate:** 6.00% (600 / 10,000)\n- **Relative lift:** +20.0% increase\n- **P-value:** 0.0003\n- **Confidence level:** 95%\n\n### \ud83d\udcdd What This Means\n\nWith 95% confidence, the difference is statistically significant.\nThe variant shows a **20.0%** improvement over control.\n</code></pre></p>"},{"location":"getting-started/quickstart/#use-the-web-interface","title":"Use the Web Interface","text":"<p>For a visual, interactive experience:</p> <pre><code>pyexptest-server\n# Open http://localhost:8000\n</code></pre> <p>The web interface includes:</p> <ul> <li>Sample Size Calculator \u2014 With explanations for each parameter</li> <li>A/B Test Results \u2014 For 2-variant and multi-variant tests</li> <li>Timing &amp; Rates \u2014 Survival analysis and Poisson comparisons</li> <li>Diff-in-Diff \u2014 Quasi-experimental analysis</li> <li>Confidence Intervals \u2014 Precision estimation</li> </ul>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Sample Size Calculator - Plan your tests properly</li> <li>Analyzing Results - Deep dive into analysis</li> <li>Multi-Variant Tests - Test 3+ variants at once</li> <li>Timing Effects - Survival and rate analysis</li> <li>API Reference - Full function documentation</li> </ul>"},{"location":"guide/analyzing-results/","title":"Analyzing Results","text":"<p>Once your test has collected enough data, it's time to analyze the results.</p>"},{"location":"guide/analyzing-results/#basic-analysis","title":"Basic Analysis","text":""},{"location":"guide/analyzing-results/#conversion-rate-test","title":"Conversion Rate Test","text":"<pre><code>from pyexptest import conversion\n\nresult = conversion.analyze(\n    control_visitors=10000,\n    control_conversions=500,      # 5.0%\n    variant_visitors=10000,\n    variant_conversions=600,      # 6.0%\n)\n\n# Key metrics\nprint(f\"Control rate: {result.control_rate:.2%}\")\nprint(f\"Variant rate: {result.variant_rate:.2%}\")\nprint(f\"Lift: {result.lift_percent:+.1f}%\")\nprint(f\"P-value: {result.p_value:.4f}\")\nprint(f\"Significant: {result.is_significant}\")\nprint(f\"Winner: {result.winner}\")\n</code></pre>"},{"location":"guide/analyzing-results/#revenue-test","title":"Revenue Test","text":"<pre><code>from pyexptest import magnitude\n\nresult = magnitude.analyze(\n    control_visitors=5000,\n    control_mean=50.00,\n    control_std=25.00,\n    variant_visitors=5000,\n    variant_mean=52.50,\n    variant_std=25.00,\n)\n\nprint(f\"Lift: ${result.lift_absolute:+.2f} ({result.lift_percent:+.1f}%)\")\nprint(f\"Winner: {result.winner}\")\n</code></pre>"},{"location":"guide/analyzing-results/#understanding-the-results","title":"Understanding the Results","text":""},{"location":"guide/analyzing-results/#is-it-significant","title":"Is It Significant?","text":"<p>Check <code>result.is_significant</code>:</p> <pre><code>if result.is_significant:\n    print(f\"Winner: {result.winner}\")\nelse:\n    print(\"No winner yet - need more data\")\n</code></pre> <p>What 'significant' means</p> <p>Statistical significance means the observed difference is unlikely to be due to random chance. It does NOT mean the difference is large or important.</p>"},{"location":"guide/analyzing-results/#reading-the-p-value","title":"Reading the P-Value","text":"<p>The p-value tells you the probability of seeing your results if there was no real difference:</p> P-value Interpretation &lt; 0.01 Very strong evidence 0.01 - 0.05 Strong evidence (significant at 95% confidence) 0.05 - 0.10 Weak evidence &gt; 0.10 Not enough evidence"},{"location":"guide/analyzing-results/#confidence-intervals","title":"Confidence Intervals","text":"<p>The confidence interval tells you the likely range of the true effect:</p> <pre><code>print(f\"95% CI: [{result.confidence_interval_lower:.4f}, {result.confidence_interval_upper:.4f}]\")\n</code></pre> <p>If the CI doesn't include zero, the result is significant.</p>"},{"location":"guide/analyzing-results/#the-recommendation","title":"The Recommendation","text":"<p>Every result includes a plain-English recommendation:</p> <pre><code>print(result.recommendation)\n</code></pre> <p>Example output: <pre><code>**Test variant is significantly higher than control** (p-value: 0.0003).\n\n_What this means:_ With 95% confidence, the difference between variant (6.00%)\nand control (5.00%) is statistically real, not due to random chance. A p-value\nof 0.0003 means there's only a 0.03% probability this result occurred by chance.\n</code></pre></p>"},{"location":"guide/analyzing-results/#generating-reports","title":"Generating Reports","text":"<p>Create shareable reports for stakeholders:</p> <pre><code>report = conversion.summarize(\n    result,\n    test_name=\"Homepage CTA Test\"\n)\nprint(report)\n</code></pre> <p>For revenue tests, customize the metric name and currency:</p> <pre><code>report = magnitude.summarize(\n    result,\n    test_name=\"Checkout Flow Test\",\n    metric_name=\"Average Order Value\",\n    currency=\"\u20ac\"\n)\n</code></pre>"},{"location":"guide/analyzing-results/#common-scenarios","title":"Common Scenarios","text":""},{"location":"guide/analyzing-results/#scenario-1-clear-winner","title":"Scenario 1: Clear Winner","text":"<pre><code>result.is_significant  # True\nresult.winner          # \"variant\"\nresult.lift_percent    # +20.0%\n</code></pre> <p>Action: Implement the variant!</p>"},{"location":"guide/analyzing-results/#scenario-2-no-significant-difference","title":"Scenario 2: No Significant Difference","text":"<pre><code>result.is_significant  # False\nresult.p_value         # 0.35\n</code></pre> <p>Action: Either: - Run the test longer to collect more data - Accept that the variants are equivalent - The effect may be smaller than you designed for</p>"},{"location":"guide/analyzing-results/#scenario-3-significant-but-small-effect","title":"Scenario 3: Significant, But Small Effect","text":"<pre><code>result.is_significant  # True\nresult.lift_percent    # +1.2%\n</code></pre> <p>Action: Consider if a 1.2% improvement is worth the implementation effort.</p>"},{"location":"guide/analyzing-results/#scenario-4-control-wins","title":"Scenario 4: Control Wins","text":"<pre><code>result.is_significant  # True\nresult.winner          # \"control\"\nresult.lift_percent    # -15.0%\n</code></pre> <p>Action: Don't implement the variant\u2014it hurts performance!</p>"},{"location":"guide/analyzing-results/#best-practices","title":"Best Practices","text":"<ol> <li>Don't peek - Decide your sample size before starting and stick to it</li> <li>Run full weeks - Capture day-of-week patterns</li> <li>Look at confidence intervals - They tell you the range of possible effects</li> <li>Consider business impact - Statistical significance \u2260 business significance</li> </ol>"},{"location":"guide/confidence-intervals/","title":"Confidence Intervals","text":"<p>Confidence intervals tell you the range where your true metric likely falls. They're often more useful than just knowing if a result is \"significant.\"</p>"},{"location":"guide/confidence-intervals/#why-confidence-intervals-matter","title":"Why Confidence Intervals Matter","text":"<p>A p-value only tells you \"is there a difference?\"</p> <p>A confidence interval tells you \"how big is the difference likely to be?\"</p> <p>Example</p> <p>Two tests both have p &lt; 0.05 (significant):</p> <ul> <li>Test A: Lift = +5%, CI = [+0.1%, +9.9%] - Could be tiny or substantial</li> <li>Test B: Lift = +5%, CI = [+4.2%, +5.8%] - Reliably around 5%</li> </ul> <p>The confidence interval gives you much more information!</p>"},{"location":"guide/confidence-intervals/#single-rate-confidence-interval","title":"Single Rate Confidence Interval","text":"<p>Get the range for a single conversion rate:</p> <pre><code>from pyexptest import conversion\n\nci = conversion.confidence_interval(\n    visitors=1000,\n    conversions=50,\n    confidence=95,\n)\n\nprint(f\"Rate: {ci.rate:.2%}\")\nprint(f\"95% CI: [{ci.lower:.2%}, {ci.upper:.2%}]\")\nprint(f\"Margin of error: \u00b1{ci.margin_of_error:.2%}\")\n</code></pre> <p>Output: <pre><code>Rate: 5.00%\n95% CI: [3.81%, 6.51%]\nMargin of error: \u00b11.35%\n</code></pre></p>"},{"location":"guide/confidence-intervals/#single-mean-confidence-interval","title":"Single Mean Confidence Interval","text":"<p>Get the range for a numeric metric:</p> <pre><code>from pyexptest import magnitude\n\nci = magnitude.confidence_interval(\n    visitors=1000,\n    mean=50.00,\n    std=25.00,\n    confidence=95,\n)\n\nprint(f\"Mean: ${ci.mean:.2f}\")\nprint(f\"95% CI: [${ci.lower:.2f}, ${ci.upper:.2f}]\")\n</code></pre>"},{"location":"guide/confidence-intervals/#confidence-intervals-in-test-results","title":"Confidence Intervals in Test Results","text":"<p>Every test result includes confidence intervals for the lift:</p> <pre><code>result = conversion.analyze(...)\n\nprint(f\"Lift: {result.lift_absolute:.4f}\")\nprint(f\"CI: [{result.confidence_interval_lower:.4f}, {result.confidence_interval_upper:.4f}]\")\n</code></pre>"},{"location":"guide/confidence-intervals/#interpreting-the-ci","title":"Interpreting the CI","text":"CI Range Interpretation Both bounds positive Variant is better (significant positive effect) Both bounds negative Control is better (significant negative effect) Spans zero No significant difference"},{"location":"guide/confidence-intervals/#effect-of-sample-size","title":"Effect of Sample Size","text":"<p>Larger samples give narrower (more precise) confidence intervals:</p> <pre><code># Small sample\nci_small = conversion.confidence_interval(visitors=100, conversions=5)\nprint(f\"n=100: [{ci_small.lower:.2%}, {ci_small.upper:.2%}]\")\n\n# Large sample\nci_large = conversion.confidence_interval(visitors=10000, conversions=500)\nprint(f\"n=10000: [{ci_large.lower:.2%}, {ci_large.upper:.2%}]\")\n</code></pre> <p>Output: <pre><code>n=100: [2.16%, 11.18%]   (9% wide)\nn=10000: [4.60%, 5.42%]  (0.8% wide)\n</code></pre></p>"},{"location":"guide/confidence-intervals/#choosing-confidence-level","title":"Choosing Confidence Level","text":"<p>Common choices:</p> Level Z-score Use When 90% 1.645 Exploratory analysis 95% 1.96 Standard for most tests 99% 2.576 High-stakes decisions <pre><code>ci_90 = conversion.confidence_interval(visitors=1000, conversions=50, confidence=90)\nci_95 = conversion.confidence_interval(visitors=1000, conversions=50, confidence=95)\nci_99 = conversion.confidence_interval(visitors=1000, conversions=50, confidence=99)\n\nprint(f\"90% CI: [{ci_90.lower:.2%}, {ci_90.upper:.2%}]\")\nprint(f\"95% CI: [{ci_95.lower:.2%}, {ci_95.upper:.2%}]\")\nprint(f\"99% CI: [{ci_99.lower:.2%}, {ci_99.upper:.2%}]\")\n</code></pre> <p>Higher confidence = wider interval.</p>"},{"location":"guide/confidence-intervals/#methods-used","title":"Methods Used","text":""},{"location":"guide/confidence-intervals/#for-conversion-rates","title":"For Conversion Rates","text":"<p>pyexptest uses the Wilson score interval, which is more accurate than the normal approximation for: - Small samples - Rates near 0% or 100%</p>"},{"location":"guide/confidence-intervals/#for-numeric-metrics","title":"For Numeric Metrics","text":"<p>pyexptest uses the t-distribution, which accounts for uncertainty in the standard deviation estimate.</p>"},{"location":"guide/confidence-intervals/#best-practices","title":"Best Practices","text":"<ol> <li>Always look at CIs - Not just p-values</li> <li>Consider practical significance - Is the lower bound of the CI large enough to matter?</li> <li>Use appropriate sample sizes - Wider CIs = less certainty</li> <li>Report CIs in presentations - They're more informative than p-values alone</li> </ol>"},{"location":"guide/diff-in-diff/","title":"Difference-in-Differences (DiD)","text":"<p>Difference-in-Differences is a statistical technique used to estimate causal effects in quasi-experimental designs. It's particularly useful when randomization isn't possible.</p>"},{"location":"guide/diff-in-diff/#when-to-use-did","title":"When to Use DiD","text":"<p>\u2705 Good use cases:</p> <ul> <li>Policy changes that affect some regions/groups but not others</li> <li>Feature rollouts to specific user segments</li> <li>Natural experiments (e.g., regulatory changes)</li> <li>Historical data analysis where you can't randomize</li> </ul> <p>\u274c Not appropriate when:</p> <ul> <li>You can run a proper randomized A/B test</li> <li>The \"parallel trends\" assumption is clearly violated</li> <li>There's no good control group</li> </ul>"},{"location":"guide/diff-in-diff/#the-did-formula","title":"The DiD Formula","text":"<p>The DiD estimator removes time trends that affect both groups:</p> <pre><code>DiD = (Treatment_post - Treatment_pre) - (Control_post - Control_pre)\n</code></pre> <p>This isolates the treatment effect by subtracting out the natural trend observed in the control group.</p>"},{"location":"guide/diff-in-diff/#conversion-rate-did","title":"Conversion Rate DiD","text":""},{"location":"guide/diff-in-diff/#basic-example","title":"Basic Example","text":"<pre><code>from pyexptest import conversion\n\n# You launched a new feature to West Coast users\n# East Coast users serve as the control group\n\nresult = conversion.diff_in_diff(\n    # Control (East Coast) - no feature\n    control_pre_visitors=10000,\n    control_pre_conversions=500,     # 5% before\n    control_post_visitors=10000,\n    control_post_conversions=525,    # 5.25% after (natural trend)\n\n    # Treatment (West Coast) - got the feature\n    treatment_pre_visitors=10000,\n    treatment_pre_conversions=500,   # 5% before\n    treatment_post_visitors=10000,\n    treatment_post_conversions=650,  # 6.5% after\n)\n\nprint(f\"Control change: {result.control_change:+.2%}\")\nprint(f\"Treatment change: {result.treatment_change:+.2%}\")\nprint(f\"DiD effect: {result.diff_in_diff:+.2%}\")\nprint(f\"P-value: {result.p_value:.4f}\")\nprint(f\"Significant: {result.is_significant}\")\n</code></pre> <p>Output: <pre><code>Control change: +0.25%\nTreatment change: +1.50%\nDiD effect: +1.25%\nP-value: 0.0012\nSignificant: True\n</code></pre></p>"},{"location":"guide/diff-in-diff/#interpreting-results","title":"Interpreting Results","text":"<p>The DiD effect (+1.25%) represents the causal impact of the treatment:</p> <ul> <li>Treatment group improved by 1.50%</li> <li>Control group improved by 0.25% (natural trend)</li> <li>Net treatment effect: 1.50% - 0.25% = 1.25%</li> </ul> <p>Without DiD, you might have claimed a 1.50% improvement, but 0.25% of that was just a natural trend!</p>"},{"location":"guide/diff-in-diff/#revenuenumeric-did","title":"Revenue/Numeric DiD","text":""},{"location":"guide/diff-in-diff/#basic-example_1","title":"Basic Example","text":"<pre><code>from pyexptest import magnitude\n\n# Testing a premium checkout experience\n# Rolled out to \"Gold\" tier customers first\n\nresult = magnitude.diff_in_diff(\n    # Control (Silver customers) - standard checkout\n    control_pre_n=2000,\n    control_pre_mean=75.00,\n    control_pre_std=30.00,\n    control_post_n=2000,\n    control_post_mean=77.00,      # $2 natural increase\n    control_post_std=32.00,\n\n    # Treatment (Gold customers) - premium checkout\n    treatment_pre_n=1500,\n    treatment_pre_mean=120.00,\n    treatment_pre_std=45.00,\n    treatment_post_n=1500,\n    treatment_post_mean=130.00,   # $10 increase\n    treatment_post_std=48.00,\n)\n\nprint(f\"Control change: ${result.control_change:+.2f}\")\nprint(f\"Treatment change: ${result.treatment_change:+.2f}\")\nprint(f\"DiD effect: ${result.diff_in_diff:+.2f}\")\nprint(f\"Significant: {result.is_significant}\")\n</code></pre> <p>Output: <pre><code>Control change: +$2.00\nTreatment change: +$10.00\nDiD effect: +$8.00\nSignificant: True\n</code></pre></p>"},{"location":"guide/diff-in-diff/#generating-reports","title":"Generating Reports","text":""},{"location":"guide/diff-in-diff/#conversion-rate-report","title":"Conversion Rate Report","text":"<pre><code>report = conversion.summarize_diff_in_diff(\n    result,\n    test_name=\"West Coast Feature Launch\"\n)\nprint(report)\n</code></pre> <p>Output: <pre><code>## \ud83d\udcca West Coast Feature Launch\n\n### \u2705 Significant Treatment Effect\n\n**The treatment caused a significant increase in conversion rate.**\n\n### Conversion Rates\n\n| Group | Pre-Period | Post-Period | Change |\n|-------|------------|-------------|--------|\n| Control | 5.00% | 5.25% | +0.25% |\n| Treatment | 5.00% | 6.50% | +1.50% |\n\n### Difference-in-Differences Estimate\n\n- **DiD Effect:** +1.25% (+25.0% relative)\n- **95% CI:** [0.52%, 1.98%]\n- **Z-statistic:** 3.35\n- **P-value:** 0.0008\n- **Confidence level:** 95%\n\n### \ud83d\udcdd What This Means\n\nThe treatment group's conversion rate changed by **+1.50%**\nwhile the control group changed by **+0.25%**.\nAfter accounting for the control group's trend, the treatment effect is **+1.25%**.\nThis effect is statistically significant at the 95% confidence level.\n</code></pre></p>"},{"location":"guide/diff-in-diff/#revenue-report","title":"Revenue Report","text":"<pre><code>report = magnitude.summarize_diff_in_diff(\n    result,\n    test_name=\"Premium Checkout Analysis\",\n    metric_name=\"Average Order Value\",\n    currency=\"$\"\n)\nprint(report)\n</code></pre>"},{"location":"guide/diff-in-diff/#the-parallel-trends-assumption","title":"The Parallel Trends Assumption","text":"<p>Critical Assumption</p> <p>DiD assumes that without the treatment, both groups would have followed similar trends. This is called the \"parallel trends\" assumption.</p>"},{"location":"guide/diff-in-diff/#checking-parallel-trends","title":"Checking Parallel Trends","text":"<p>Before applying DiD, verify that:</p> <ol> <li>Historical trends are similar: Plot both groups' metrics over time before the treatment</li> <li>No anticipation effects: The treatment group didn't change behavior before the treatment started</li> <li>No contamination: Control group wasn't affected by the treatment spillover</li> </ol>"},{"location":"guide/diff-in-diff/#what-violates-parallel-trends","title":"What Violates Parallel Trends","text":"<ul> <li>Seasonality differences: One region has different seasonal patterns</li> <li>Selection bias: Treatment group was chosen because they were already improving</li> <li>Confounding events: Something else happened to one group at the same time</li> </ul>"},{"location":"guide/diff-in-diff/#best-practices","title":"Best Practices","text":"<ol> <li>Collect enough pre-period data - Multiple time points help validate parallel trends</li> <li>Choose a similar control group - The more similar, the better</li> <li>Check for spillover effects - Make sure control isn't affected by treatment</li> <li>Report confidence intervals - They show the uncertainty in your estimate</li> <li>Consider placebo tests - Apply DiD to periods before treatment as a sanity check</li> </ol>"},{"location":"guide/diff-in-diff/#limitations","title":"Limitations","text":"<ol> <li>Cannot prove causation - DiD is correlational; parallel trends may not hold</li> <li>Sensitive to timing - Results can vary based on when you measure</li> <li>Assumes linear trends - Non-linear dynamics may bias estimates</li> <li>Requires good control group - Hard to find in practice</li> </ol>"},{"location":"guide/diff-in-diff/#when-did-beats-ab-testing","title":"When DiD Beats A/B Testing","text":"Scenario Use DiD Use A/B Test Can randomize \u274c \u2705 Policy/regulation change \u2705 \u274c Historical analysis \u2705 \u274c Feature rollout with holdout \u2705 \u2705 Need causal certainty \u274c \u2705"},{"location":"guide/multi-variant/","title":"Multi-Variant Tests","text":"<p>Sometimes you want to test more than one variant at a time. pyexptest supports multi-variant testing with proper statistical adjustments.</p>"},{"location":"guide/multi-variant/#when-to-use-multi-variant-tests","title":"When to Use Multi-Variant Tests","text":"<p>\u2705 Good use cases: - Testing 2-3 different designs - Comparing multiple pricing options - Testing several copy variations</p> <p>\u274c Bad use cases: - Testing 10+ variants (need too much traffic) - Testing unrelated changes (run separate tests)</p>"},{"location":"guide/multi-variant/#planning-a-multi-variant-test","title":"Planning a Multi-Variant Test","text":"<p>Multi-variant tests need more sample size:</p> <pre><code>from pyexptest import conversion\n\n# 2-variant test\nplan_2 = conversion.sample_size(current_rate=5, lift_percent=10, num_variants=2)\nprint(f\"2 variants: {plan_2.total_visitors:,} total\")\n\n# 3-variant test\nplan_3 = conversion.sample_size(current_rate=5, lift_percent=10, num_variants=3)\nprint(f\"3 variants: {plan_3.total_visitors:,} total\")\n\n# 4-variant test\nplan_4 = conversion.sample_size(current_rate=5, lift_percent=10, num_variants=4)\nprint(f\"4 variants: {plan_4.total_visitors:,} total\")\n</code></pre> <p>Output: <pre><code>2 variants: 62,468 total\n3 variants: 108,702 total  (+74%)\n4 variants: 157,872 total  (+153%)\n</code></pre></p> <p>Traffic Requirements</p> <p>Each additional variant significantly increases the required sample size. Stick to 3-4 variants max.</p>"},{"location":"guide/multi-variant/#analyzing-conversion-rate-tests","title":"Analyzing Conversion Rate Tests","text":"<p>Use Chi-square test for conversion rate multi-variant tests:</p> <pre><code>from pyexptest import conversion\n\nresult = conversion.analyze_multi(\n    variants=[\n        {\"name\": \"control\", \"visitors\": 10000, \"conversions\": 500},\n        {\"name\": \"red_button\", \"visitors\": 10000, \"conversions\": 550},\n        {\"name\": \"green_button\", \"visitors\": 10000, \"conversions\": 600},\n        {\"name\": \"blue_button\", \"visitors\": 10000, \"conversions\": 480},\n    ]\n)\n\nprint(f\"Overall significant: {result.is_significant}\")\nprint(f\"Best variant: {result.best_variant}\")\nprint(f\"P-value: {result.p_value:.4f}\")\n</code></pre>"},{"location":"guide/multi-variant/#understanding-the-results","title":"Understanding the Results","text":"<p>The multi-variant test has two levels:</p> <ol> <li>Overall test (Chi-square): Is there ANY difference between variants?</li> <li>Pairwise comparisons: WHICH variants are different from each other?</li> </ol> <pre><code># Overall test\nprint(f\"Chi-square statistic: {result.test_statistic:.2f}\")\nprint(f\"P-value: {result.p_value:.4f}\")\n\n# Pairwise comparisons\nfor p in result.pairwise_comparisons:\n    status = \"\u2713\" if p.is_significant else \" \"\n    print(f\"{status} {p.variant_a} vs {p.variant_b}: {p.lift_percent:+.1f}% (p={p.p_value_adjusted:.4f})\")\n</code></pre>"},{"location":"guide/multi-variant/#analyzing-revenue-tests","title":"Analyzing Revenue Tests","text":"<p>Use ANOVA for numeric metric multi-variant tests:</p> <pre><code>from pyexptest import magnitude\n\nresult = magnitude.analyze_multi(\n    variants=[\n        {\"name\": \"control\", \"visitors\": 1000, \"mean\": 50, \"std\": 25},\n        {\"name\": \"simple_checkout\", \"visitors\": 1000, \"mean\": 52, \"std\": 25},\n        {\"name\": \"premium_upsell\", \"visitors\": 1000, \"mean\": 55, \"std\": 25},\n    ]\n)\n\nprint(f\"F-statistic: {result.f_statistic:.2f}\")\nprint(f\"Best variant: {result.best_variant}\")\n</code></pre>"},{"location":"guide/multi-variant/#bonferroni-correction","title":"Bonferroni Correction","text":"<p>When making multiple comparisons, we adjust p-values to avoid false positives:</p> <pre><code># With correction (default)\nresult = conversion.analyze_multi(variants, correction=\"bonferroni\")\n\n# Without correction (not recommended)\nresult = conversion.analyze_multi(variants, correction=\"none\")\n</code></pre> <p>Why Bonferroni?</p> <p>Testing 3 variants means 3 pairwise comparisons. Without correction, you have a ~14% chance of a false positive instead of 5%. Bonferroni adjusts p-values to maintain the 5% overall false positive rate.</p>"},{"location":"guide/multi-variant/#generating-reports","title":"Generating Reports","text":"<pre><code>report = conversion.summarize_multi(\n    result,\n    test_name=\"Button Color Test\"\n)\nprint(report)\n</code></pre> <p>Output: <pre><code>## \ud83d\udcca Button Color Test Results\n\n### \u2705 Significant Differences Detected\n\n**At least one variant performs differently from the others.**\n\n### Variant Performance\n\n| Variant | Visitors | Conversions | Rate |\n|---------|----------|-------------|------|\n| green_button \ud83c\udfc6 | 10,000 | 600 | 6.00% |\n| red_button | 10,000 | 550 | 5.50% |\n| control | 10,000 | 500 | 5.00% |\n| blue_button | 10,000 | 480 | 4.80% |\n\n### Overall Test (Chi-Square)\n\n- **Test statistic:** 27.45\n- **Degrees of freedom:** 3\n- **P-value:** 0.0001\n- **Confidence level:** 95%\n\n### Significant Pairwise Differences\n\n- **green_button** beats **control** by 20.0% (p=0.0003)\n- **green_button** beats **blue_button** by 25.0% (p=0.0001)\n- **red_button** beats **blue_button** by 14.6% (p=0.0234)\n\n### \ud83d\udcdd What This Means\n\nWith 95% confidence, there are real differences between your variants.\n**green_button** has the highest conversion rate.\n</code></pre></p>"},{"location":"guide/multi-variant/#best-practices","title":"Best Practices","text":"<ol> <li>Limit variants - Stick to 3-4 variants max</li> <li>Use Bonferroni - Always use correction for pairwise comparisons</li> <li>Plan traffic - Calculate sample size before starting</li> <li>Overall first - If overall test isn't significant, don't trust pairwise comparisons</li> <li>Pre-register - Decide which comparisons matter before seeing results</li> </ol>"},{"location":"guide/p-values/","title":"Understanding P-Values","text":"<p>P-values are widely used but often misunderstood. This guide explains what they actually mean and how to interpret them correctly.</p>"},{"location":"guide/p-values/#what-a-p-value-is","title":"What a P-Value Is","text":"<p>The p-value is the probability of seeing results as extreme as yours if there was no real difference between control and variant.</p> <p>In Plain English</p> <p>\"If the variant was actually identical to control, how likely would I be to see this result just by chance?\"</p>"},{"location":"guide/p-values/#interpreting-p-values","title":"Interpreting P-Values","text":"P-value Interpretation &lt; 0.01 Very strong evidence of a real difference 0.01 - 0.05 Strong evidence (significant at 95% confidence) 0.05 - 0.10 Weak evidence, consider more data &gt; 0.10 Not enough evidence to conclude there's a difference"},{"location":"guide/p-values/#example","title":"Example","text":"<pre><code>from pyexptest import conversion\n\nresult = conversion.analyze(\n    control_visitors=10000,\n    control_conversions=500,    # 5.0%\n    variant_visitors=10000,\n    variant_conversions=600,    # 6.0%\n)\n\nprint(f\"P-value: {result.p_value:.4f}\")\n</code></pre> <p>Output: <code>P-value: 0.0003</code></p> <p>Interpretation: There's only a 0.03% chance of seeing a 1 percentage point difference (or larger) if the variant was actually the same as control. This is very unlikely, so we conclude the variant really is different.</p>"},{"location":"guide/p-values/#what-p-values-do-not-mean","title":"What P-Values Do NOT Mean","text":"<p>\u274c Wrong: \"There's a 0.03% chance the variant isn't better\"</p> <p>\u274c Wrong: \"The variant is 99.97% likely to be better\"</p> <p>\u274c Wrong: \"The effect is large\"</p> <p>\u2705 Right: \"If there was no real difference, we'd only see results this extreme 0.03% of the time\"</p>"},{"location":"guide/p-values/#the-005-threshold","title":"The 0.05 Threshold","text":"<p>The conventional threshold is p &lt; 0.05 (5%), which corresponds to 95% confidence.</p> <pre><code>if result.p_value &lt; 0.05:\n    print(\"Statistically significant at 95% confidence\")\nelse:\n    print(\"Not statistically significant\")\n</code></pre> <p>0.05 is arbitrary</p> <p>The 0.05 threshold is a convention, not a law of nature. A p-value of 0.051 isn't meaningfully different from 0.049.</p>"},{"location":"guide/p-values/#relationship-to-confidence-level","title":"Relationship to Confidence Level","text":"Confidence Level P-value Threshold 90% &lt; 0.10 95% &lt; 0.05 99% &lt; 0.01 <pre><code># 95% confidence (default)\nresult_95 = conversion.analyze(..., confidence=95)\n# Significant if p &lt; 0.05\n\n# 99% confidence\nresult_99 = conversion.analyze(..., confidence=99)\n# Significant if p &lt; 0.01\n</code></pre>"},{"location":"guide/p-values/#common-mistakes","title":"Common Mistakes","text":""},{"location":"guide/p-values/#mistake-1-peeking-and-stopping-early","title":"Mistake 1: Peeking and Stopping Early","text":"<p>Problem: Checking results daily and stopping when p &lt; 0.05.</p> <p>Why it's wrong: The more you check, the more likely you'll see p &lt; 0.05 by chance.</p> <p>Solution: Calculate sample size before starting and don't peek.</p>"},{"location":"guide/p-values/#mistake-2-ignoring-effect-size","title":"Mistake 2: Ignoring Effect Size","text":"<p>Problem: A test shows p = 0.01 with a 0.1% lift.</p> <p>Why it's wrong: Statistical significance doesn't mean business significance.</p> <p>Solution: Always look at confidence intervals and consider if the effect is worth implementing.</p>"},{"location":"guide/p-values/#mistake-3-multiple-comparisons","title":"Mistake 3: Multiple Comparisons","text":"<p>Problem: Running 20 tests and declaring 1 winner (p = 0.04).</p> <p>Why it's wrong: With 20 tests, you expect ~1 false positive at p &lt; 0.05.</p> <p>Solution: Use Bonferroni correction for multi-variant tests.</p>"},{"location":"guide/p-values/#p-value-vs-confidence-interval","title":"P-Value vs. Confidence Interval","text":"<p>P-values tell you: \"Is there a difference?\"</p> <p>Confidence intervals tell you: \"How big is the difference?\"</p> <pre><code>result = conversion.analyze(...)\n\n# P-value approach\nif result.p_value &lt; 0.05:\n    print(\"Significant!\")\n\n# CI approach (more informative)\nprint(f\"Lift: {result.lift_percent:+.1f}%\")\nprint(f\"CI: [{result.confidence_interval_lower:.4f}, {result.confidence_interval_upper:.4f}]\")\n</code></pre> <p>Best practice</p> <p>Report both the p-value AND the confidence interval. The CI tells stakeholders the likely range of the true effect.</p>"},{"location":"guide/p-values/#one-tailed-vs-two-tailed","title":"One-Tailed vs. Two-Tailed","text":"<p>pyexptest uses two-tailed tests by default, which is appropriate when you want to detect effects in either direction.</p> Test Type Detects Use When Two-tailed Effects in either direction Most A/B tests One-tailed Effects in only one direction Rarely appropriate"},{"location":"guide/p-values/#summary","title":"Summary","text":"<ol> <li>P-value = Probability of seeing your results if there's no real difference</li> <li>p &lt; 0.05 is the conventional threshold for \"significant\"</li> <li>Don't peek - Calculate sample size first</li> <li>Look at CIs - They're more informative than p-values alone</li> <li>Consider business impact - Statistical significance \u2260 business significance</li> </ol>"},{"location":"guide/sample-size/","title":"Sample Size Calculator","text":"<p>Calculating the right sample size before you start a test is crucial. Too few visitors and you won't detect real effects. Too many and you're wasting time.</p>"},{"location":"guide/sample-size/#why-sample-size-matters","title":"Why Sample Size Matters","text":"<p>Don't Skip This Step</p> <p>Running a test without calculating sample size first is the #1 cause of inconclusive results.</p> <p>A proper sample size calculation ensures:</p> <ol> <li>High enough power - You'll detect real effects when they exist</li> <li>Controlled false positive rate - You won't declare winners that aren't real</li> <li>Efficient use of traffic - You won't run tests longer than necessary</li> </ol>"},{"location":"guide/sample-size/#for-conversion-rate-tests","title":"For Conversion Rate Tests","text":"<pre><code>from pyexptest import conversion\n\nplan = conversion.sample_size(\n    current_rate=5,       # Your current conversion rate (5%)\n    lift_percent=10,      # Minimum lift you want to detect (10%)\n    confidence=95,        # Confidence level (default: 95%)\n    power=80,             # Statistical power (default: 80%)\n)\n\nprint(f\"Visitors per variant: {plan.visitors_per_variant:,}\")\nprint(f\"Total visitors: {plan.total_visitors:,}\")\n</code></pre>"},{"location":"guide/sample-size/#understanding-the-parameters","title":"Understanding the Parameters","text":"Parameter What It Means Typical Values <code>current_rate</code> Your baseline conversion rate 1-10% for most sites <code>lift_percent</code> Smallest improvement worth detecting 5-20% relative lift <code>confidence</code> How sure you want to be (avoids false positives) 95% (standard) <code>power</code> Chance of detecting a real effect 80% (standard)"},{"location":"guide/sample-size/#estimating-test-duration","title":"Estimating Test Duration","text":"<p>Add your daily traffic to get a duration estimate:</p> <pre><code>plan.with_daily_traffic(10000)  # 10k visitors/day\nprint(f\"Test duration: {plan.test_duration_days} days\")\n</code></pre> <p>Run for at least 1-2 weeks</p> <p>Even if you reach sample size sooner, run for at least a week to capture day-of-week effects.</p>"},{"location":"guide/sample-size/#for-revenue-numeric-tests","title":"For Revenue / Numeric Tests","text":"<pre><code>from pyexptest import magnitude\n\nplan = magnitude.sample_size(\n    current_mean=50,      # Current average order value ($50)\n    current_std=25,       # Standard deviation ($25)\n    lift_percent=5,       # Minimum lift you want to detect (5%)\n)\n\nprint(f\"Visitors per variant: {plan.visitors_per_variant:,}\")\n</code></pre>"},{"location":"guide/sample-size/#getting-standard-deviation","title":"Getting Standard Deviation","text":"<p>Don't know your standard deviation? Here's how to estimate it:</p> <pre><code>-- SQL example\nSELECT STDDEV(order_value) FROM orders WHERE date &gt; '2024-01-01';\n</code></pre> <p>Or use this rule of thumb:</p> <ul> <li>For revenue: std is typically 50-100% of the mean</li> <li>For time metrics: std is typically 100-200% of the mean</li> </ul>"},{"location":"guide/sample-size/#for-multi-variant-tests","title":"For Multi-Variant Tests","text":"<p>Testing 3+ variants requires more sample size:</p> <pre><code>plan = conversion.sample_size(\n    current_rate=5,\n    lift_percent=10,\n    num_variants=3,  # Control + 2 variants\n)\n\nprint(f\"Per variant: {plan.visitors_per_variant:,}\")\nprint(f\"Total: {plan.total_visitors:,}\")  # ~50% more than 2-variant\n</code></pre>"},{"location":"guide/sample-size/#sample-size-trade-offs","title":"Sample Size Trade-offs","text":"If You Want To... You Need... Detect smaller effects More visitors Higher confidence (99% vs 95%) More visitors Higher power (90% vs 80%) More visitors Test more variants More visitors per variant"},{"location":"guide/sample-size/#generating-a-report","title":"Generating a Report","text":"<p>Create a stakeholder-friendly plan:</p> <pre><code>report = conversion.summarize_plan(\n    plan, \n    test_name=\"Homepage CTA Test\"\n)\nprint(report)\n</code></pre> <p>Output: <pre><code>## \ud83d\udccb Homepage CTA Test Sample Size Plan\n\n### Test Parameters\n\n- **Current conversion rate:** 5.00%\n- **Minimum detectable lift:** +10%\n- **Expected variant rate:** 5.50%\n- **Confidence level:** 95%\n- **Statistical power:** 80%\n\n### Required Sample Size\n\n- **Per variant:** 31,234 visitors\n- **Total:** 62,468 visitors\n\n### \ud83d\udcdd What This Means\n\nIf the variant truly improves conversion by 10% or more,\nthis test has an 80% chance of detecting it.\n</code></pre></p>"}]}